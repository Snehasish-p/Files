#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#

Introduction to Digital Transformation with Google Cloud

1.1 Overview
Innovation doesn't come in a linear way, it comes in waves, and each of these waves is powered by a breakthrough technology. There was the age of the printing press, the steam engine, electricity, the transportation age, the first computers, and today, data science, also known as the age of cloud technology. Each of these inventions triggered thousands of innovations in what are called Kondratiev Waves or innovation waves.
We're right in the middle of another paradigm shift. Cloud technology is transforming how businesses create value, how people work, and ultimately how people live. As with any other innovation, wave Cloud technology is generating thousands of new innovations such as chat bots and predictive medicine. This is a branch of medicine that aims to identify patients at risk of developing a disease, thereby enabling either prevention or early treatment of that disease.



1.2 A new cloud paradigm

What is cloud?
The Cloud is a metaphor for the network of data centers that store and compute information available through the Internet. Essentially, instead of describing a complex web of software, computers, network, and security systems, all of that has been combined into one word, Cloud. 
When we talk about Cloud computing, we're talking about the technology and processes needed to store, manage, and access data that is transferred over the Cloud rather than the data that remains on your computers hard drive. 
Companies such as Google Cloud have invested heavily in their own IT infrastructure, creating vast digital spaces to store and process data. Now they're helping other organizations around the world by offering them the use of their digital platform to run their applications at scale. This has generated massive cost reductions for companies that operated data warehouses without economies of scale and allowed software developers around the world to access well-established IT infrastructures. 

It's not meant to only be a place to store your data, it's capable of doing so much more. Cloud is revolutionary because it enables every professional to fundamentally rethink and re-imagine how they do business, from collecting data to gaining insights from it, to working with their peers globally, to serving their customers. When an organization takes advantage of new technologies such as Cloud to redesign and redefine relationships with their customers, employees, and partners, the result is a company wide digital transformation. Digital transformation is about taking advantage of established global scale IT infrastructures and leveraging vast compute power that makes it possible for developers to build revolutionary new applications. It's about a foundational change in how an organization operates and optimizes internal resources and how it delivers value to customers. 
Cloud technology can map, understand, and predict human behavior, human biology, global industrial systems, and every other complex and dynamic environment. These examples are things we could only dream of doing digitally in the past. This means that in the same way electricity powered light bulbs, radios, and computers, Cloud technology is powering a new range of applications that are continuously learning and improving. 

Cloud enables and redefines our ability to collaborate, perceive, categorize, predict, and recommend in every industry for every activity. 




Danger of maintaining the status quo
There's no greater danger for any organization than to keep the technology its always used to refine and perfect what it's always done instead of letting it go and moving on to the next technology platform. Abandoning old technology for a new one is commonly referred to as the "Burning Platform Effect." It requires organizations to take a leap of faith and to continually adapt as new technologies create new paradigm shifts. Let me illustrate what happens as a result of a burning platform effect with two concrete examples. The first company takes advantage of new technology and makes a paradigm shift, and the second doesn't. Our first example is Nintendo. Nintendo has been creating games since 1889. They started with traditional Japanese playing cards called Hanafuda, which were made possible by the printing press. From there, Nintendo has consistently used new technology to transform their business and become a leader in the gaming industry. They were even among the first to introduce gaming consoles and mobile gaming devices. Still, they didn't dwell on these successes. Instead, they revolutionized mobile gaming when they launched Pok√©mon Go in 2016. Then the first cloud gaming console, Nintendo Switch, one year later, in 2017. At a time when most of their competitors were going out of business, Nintendo jumped from one burning platform to the next, consistently maintaining and even expanding its market share and its customer base along the way. What makes Nintendo so successful at this? The answer is that they consistently focus on why they exist, not how they operate. They exist because they want people to play. Naturally, they'll use any technology as a resource to achieve this mission. If they focused on liquid crystal displays as the best tool for gaming, then each new technology would have posed a threat to them. Instead, they utilized liquid crystal displays for a while and then quickly shifted as the next technology became available to continue motivating people to play. This is an example of a company that thrives with new technology. By contrast, companies that sold encyclopedias all focused on how they operate, how to print and sell a very specific set of books. This is what they were proud of, a beautiful set of leather-covered books, lined up on the shelves of the finest libraries. Because of the high costs of these sets, only a few scholars or the elite could afford them. For businesses that made and sold encyclopedias, they needed printing machines, well-kept warehouses, bookshelf makers, a way to ship and receive heavy containers, and a good door-to-door selling mechanism. These companies became obsessed with the books and lost sight of their initial mission, to capture and share human knowledge by any means. Naturally, as new technology became available, instead of serving as an opportunity, it served as a threat to their business. The first of these threats was the CD-ROMs. Quite amazingly, all the CD-ROM-based encyclopedia providers made the exact same mistake and were later driven out of business by cloud-native applications such as Wikipedia. Nintendo and encyclopedia companies were both born from the printing era. Nintendo beginning with traditional playing cards, and encyclopedias stemmed from traditional books. Yet the two companies didn't react to their burning platform the same way, and their business has led to very different outcomes. One drives and the other no longer exists. Many other traditional industries have been disrupted in similar ways. The movie rental industry, for example, has been disrupted with On-demand streaming services. Chemical film manufacturers have been disrupted with LCD sensors and smartphone revolution. Any business leader who considers cloud technology as not relevant to them or even worse, as a threat to the way they've always done business, risks facing a fate similar to that of encyclopedia companies. Disruption or ceasing to exist entirely. Imagine the impact the slow decline of a business has on its workforce. Many jobs are lost and the remaining workforce is not skilled or able to manage the demands of the new world. The downstream impacts are significant too. Vendors and partners could face a sharp decline in revenue due to loss of business. Now, the good news is, the Cloud is still very new for many businesses. Wherever business is in the Cloud adoption journey, there's still time to catch up. The key point is that now is the time for organizations to accelerate their digital transformation and shift the way they think about their business potential.




A paradigm shift
The adoption of Cloud technology is not just about survival, it's about thriving in a new technological era. The business benefits of truly embracing a digital transformation using Cloud technology are significant. Many organizations are already realizing the business benefits of secure and fast application development and deployment. They're seeing a shift in their costs from capital expenditure to operational expenditure, or from CapEx to OpEx. In many cases, they're saving costs and generating new value. They're modernizing their infrastructure, solving business problems at a much faster speed, and gaining insights from their data. Like Nintendo, these visionary companies are leveraging new technological capabilities to reshape their customers' lives across multiple industries. When I say new technological capabilities, I'm referring to the ones that brought us innovations like the self-driving car. It has completely revolutionized the transportation industry once again, and the same is true for smart assistants, which are already changing how we behave and how quickly and easily we get things done. Think about it, smart assistants let you use your voice and the natural way you speak to get the information you need in milliseconds. In some cases, you don't even need to prompt the assistant, it proactively makes the suggestions for you. For example, suppose you're driving and need to slow down or stop because of a traffic jam. The smart assistant built into your GPS, like Google Maps, automatically suggests an alternate route and in just the right time so you can act accordingly, and there are many more ways Cloud technology is changing our lives. The Cloud's extraordinary compute capability is combined with large amounts of data to create experiences we've never known before. For example, the Internet of Things, or IoT, refers to everyday objects or devices that are connected to the Internet and are able to send and receive data. Smartphones, connected appliances, mobile sensors, and wearable devices are all IoT examples. The Internet of Things has become so present in our everyday life that we don't go online anymore, we live online, and while doing so we receive and generate vast quantities of data at every moment. This was already true with the invention of the radio or the television, but the scale and the speed at which we share data has tremendously increased, and our ability to compute that data has followed an exponential curve known as Moore's Law. The result is that, today we can build highly accurate statistical models to predict complex behaviors and use that information to anticipate intent. This is the most profound aspect of the Cloud revolution. In addition to its predictive capabilities, the Cloud delivers high-performance analytics and enables businesses to reduce equipment downtime, achieve more accurate supply planning and maintain leaner operational organizations that have more efficient systems and less waste. The power of Cloud also changes the way we work by automating processes and creating open and real-time collaboration opportunities between people globally. This includes business stakeholders, customers, students, and constituents. In all of the examples I mentioned so far, there are two common ingredients; compute power and data. Not just any compute power or a few files containing data, but extraordinary compute power and very, very large volumes of data.




1.3 Core cloud ingredients

Extraordinary compute power
Computing at its most basic is any task that requires a calculation. In the context of Cloud, computing is the ability to process information, to store, retrieve, compare and analyze it, and automate tasks most often done by a computer program. Compute power, therefore, refers to the speed at which a computer is able to process data. In the beginning, giant computers owned by institutions filled entire rooms and took a long time to process small amounts of data. But two recent changes dramatically affected the computing landscape. The first change brought computers from institutions to individuals and exponential growth trajectory that doubled computing power every other year. This was only the beginning. Moore's Law has been disrupted twice by radical new designs in chips. The first disruption comes from processors that are specifically meant for this type of application and which we call TPUs or tensor flow processing units. According to Moore's Law, TPUs should be twice as powerful as the preceding processing units, but they're not. They are 50 times more powerful than traditional chips, and that's not all. The second disruption comes from quantum computing, which is a hundred million times more powerful. To illustrate the full impact of the TPU on what organizations can now readily do, I'll use a machine learning example. First and simply put, machine learning is a way of training a computer to automatically do something using lots of data. A machine learning model that would require a day of training with traditional processors only requires half an hour of training with TPUs. Another machine learning model that would require thousands of years of training with traditional processors would now require only a few seconds with quantum computing. There is a constraint attached to this evolution, though. Right now, the cooling requirements for these new processors can only be met in large industrial environments. TPUs need to be cooled with pressurized water within the chip, while quantum computing requires absolute zero temperatures to operate. Cloud data centers are the only environment where we can create these conditions at scale. For the foreseeable future, they'll be the only option to tap into this vast amount of computing power. Think about electricity, in the first years after its discovery, most users had generators located where they lived or worked. As the industry became more mature, we created power plants that we access through a grid. The same principle applies today to computing. It will mainly be generated in plants, which are the data centers and access to a grid, which is the Internet. This brings us to the second ingredient common in the examples I covered earlier in this video, data, specifically, lots and lots of it.




Data as currency
Why is data needed to unlock the capabilities of Cloud? Traditional computing is about taking a piece of information and input through an algorithm in order to get an output. This can be a very tedious process. It requires someone to teach a computer what to do one step at a time. Sure, it can follow those steps faster than any human, but it can only do what humans have programmed it to do and nothing more. Traditional computing, therefore, follows a static process. With Cloud computing, things change. You can provide both the input and output data side by side and generate a trained algorithm. This means it uses a combination of existing input and output data to learn the correct outcome when faced with information it hasn't encountered before. It is therefore self-learning. For instance, in an image-recognition application such as Google Photos, you can provide your photos, the input, and the correct identifier of the friend or family member in the photo, so the output. Then you can train the image-recognition algorithm embedded in the app to automatically recognize and categorize new photos of your family members or friends even if their appearance goes through significant changes over time. That's the case, for instance, of a baby that grows to become a child and then eventually an adult. The app will still be able to identify the person in any new photo at every stage of their development. Another example of combining compute power and data to create new user experiences can be seen in social media apps. From personalizing your news feed to better-targeted advertising, social media platforms use machine learning to learn about user preferences and intent. This can be used to recommend people you may know for example. The embedded ML algorithm is learning from the profiles that you visit often, your interests, locations you tag in photos, your searches, or a group that you share with someone. It then compiles a recommended list of people you may want to become friends with or to follow using this information. The accuracy and performance of the algorithm is also based on the millions of users that the algorithm is constantly learning from. We'll talk more about the possible applications of machine learning in the Innervating With Data and Google Cloud course and the role that ML plays in digital transformation.




2.1 Overview
As access to Cloud computing resources has become more globally accessible and available, early adopters have been able to leverage it to create new business value. As a result, customer expectations around the world have dramatically changed. Customers now expect relevant and easily accessible content and information almost instantly. They also expect services that are always running and available anywhere in the world. No matter who your customer is, the new Cloud paradigm requires your organization to operate in new ways to meet these ever evolving and increasingly personalized expectations. In this module, we'll explore the business and technical considerations that organizations need to think about as they embrace the Cloud. Specifically, I'll begin by discussing how organizations need to adapt their ways of working to meet the needs of a global workforce and customer base. Then I'll turn to traditional IT Infrastructure challenges and how businesses should think about modernization. Then I'll explore what company wide applications are, and how have they been built historically. I'll also touch on considerations for developing and updating applications quickly, securely, and at scale. After that, I'll discuss the importance of data. This includes the inevitable challenges that organizations face in capturing, storing, and leveraging their data to gain business insights and make data driven decisions. Then I'll examine vital security considerations that must be built into an organization called Adoption Journey. Finally, I'll give an overview of Google Cloud Solutions for digital transformation. 




2.2 Cloud adoption challenges

Enhancing productivity and collaboration
One major challenge that organizations may face as they embrace the Cloud is changing the way they work. This is important for two reasons. First, because as consumer expectations change, business models must adapt to remain relevant. Second, the traditional advantages of size and scale, are no longer as differentiating as they used to be. Let me explain. Traditionally, large companies with big budgets were able to operate at scale, giving them major competitive advantage. They had the required capital to set up, maintain, and even expand their IT infrastructures, giving their employees access to the latest tools to do their jobs. However, the universal availability of Cloud Technology gives large and small companies equal opportunity for success. In fact, in many cases, by leveraging Cloud technology to serve customers in radically new ways, small businesses have been able to disrupt industries where large scale organizations have historically owned much of the market. Organizations now need to be innovative, agile, quick to market, and highly customer-focused. In other words, they have to change the way they work to adapt to these new business imperatives. At Google, we believe that focusing on innovation, productivity, and collaboration, are vital to an organization's ability to make this change. Innovation is about doing something in a surprising new way. There are many ways to encourage innovation and increase collaboration and productivity. At Google, we believe you can achieve these through company culture, and technology. Let me explain. The culture of an organization has a direct impact on employee's willingness to innovate. Innovation relies on people being able to try things and failing without judgment. It also depends on employees having access to the information they need to develop fresh, realistic ideas. How teams are structured, how content is managed, and how communication flows across an organization, are all elements that significantly affect innovation. Next, equipping people with the tools they need to succeed is also an important part of enabling innovation, and increased productivity and collaboration.




Modernizing IT infrastructure
We've looked at how organizations need to change the way they work to thrive in the Cloud and meet customers where they are. Another challenge is to transform the IT backbone with which the organization runs. This is especially challenging for large enterprises that have deeply entrenched and complex systems, hardware and processes that need to change. There are three core focus areas for modernization; infrastructure, business platforms, and applications. Let's look at each of these one by one. Infrastructure modernization is a common term used to describe the process of replacing legacy hardware and systems and consolidating them in the Cloud. This often poses many challenges for organizations from defining new governance policies through to ensuring that security systems are in place. In addition, business critical applications are often running on existing on-premises infrastructure, so transitioning them to the Cloud can be a big change. An added complexity is that most organizations want to operate a hybrid model. This means operating across multiple public Cloud providers or a mixture of private on-premises and public Cloud solutions. Integrating systems and information across multiple environments is complicated. But when an organization has embraced the process of modernization, the opportunities for business transformation are huge. Let's look at a couple of examples. Businesses can take advantage of high performance computing in a cost-effective and scalable way. Traditional IT required significant upfront expenditure to ensure that the hardware was in place for the just in case moments such as peaks in demand. With cloud computing, businesses can scale in the Cloud and pay for what they use when they use it. This has significant implications for how an organization thinks about operations and IT budgets. Another example of modernization is Virtual Desktops. Recently, the global pandemic has radically impacted how teams work together. With the rapid shift to work from home, companies are confronted with a challenge of balancing security and IT resources with the demands for working from home. Virtual desktops are pre-configured images of operating systems and applications. This means that accessing the operating system doesn't rely on the physical device running on the operating system. Instead, users can access the virtual desktops remotely using any endpoint device, such as a laptop, a smartphone, or a tablet. This enables secure and scalable access to corporate resources. Cost reductions and virtual desktops are just two examples of what businesses can do when they embrace Cloud technology and modernize their infrastructure. But infrastructure modernization is only the start. 




Modernizing business platforms and applications
Business application platforms are essentially about enabling integration between systems and granting users the correct access privileges in an organization and beyond. For example, suppose a company uses a third party platform for managing parts of customer data, customers who have purchased a premium support package should be prioritized by the partner in that region, application processing interfaces, commonly known as APIs are technical tools that enable integration between applications. In this case, the company uses an API to integrate their partner portal application with the customer information platform. They configure it so that the partner can only access certain information and other information remains protected. We'll talk about business application platforms in a later module. Now, let's look at application modernization. The term application is widely used to refer to programs and software that enable people to preform various digital tasks. Apps on smartphones are one example. Another example is the software you use to create documents, spreadsheets and presentations. Today's customers expect instant access to services wherever they are. An organization's ability to develop and launch applications is central to their success in today's competitive market. But organizations must embrace the importance of speed and innovation without compromising security. One way to realize this is through developer operations or DevOps. DevOps is a set of practices that aim to increase software delivery velocity, improve service reliability, and build shared ownership among software stakeholders. We explore application modernization, including DevOps and a lot more detail in another course, understanding Google cloud security and operations. Let's now look at how organizations can leverage data to enable digital transformation.




Unlocking the value of data
Leveraging data relies on being able to capture, store, and structure it in such a way that you can make informed business decisions with it. Data is no longer only about retrospective insight, it also includes real time insight, smart predictions, and intelligent action. Imagine you're working in a traditional large enterprise. Lots of offices around the world, tons of documents, spreadsheets, and files, varying platforms and applications, assets and materials in various languages, and a global customer base, the wealth of available data is enormous. Some data, like financial data, is easy to capture because it already lives in spreadsheets. Other data is harder to capture, like content that is spread across PDF and forms or social data. How your customers engage with you across social media platforms, for example. Another challenge is storage and data management. After you've captured data, how do you store it in such a way that you can gain insights from it? With the right platform, organizations can generate instant insights from data at any scale. Instead of analyzing data for retrospective insight, you can leverage data in real time to continually improve your service. For example, organizations can use stream analytics tools to instantly capture consumer behavior on their website and respond in a more targeted way in real time. When an organization has captured data and has systems in place to continue capturing it at scale, the possibilities are endless. With machine learning and artificial intelligence, or ML and AI, you can generate insights from data both past and present, and you can also perceive, predict, recommend, and categorize data in new ways. For example, ML enables large equipment manufacturers to schedule predictive maintenance with greater accuracy, leading to less downtime and increased productivity. Online retailers who use smart analytics tools can ingest real time behavior data while also leveraging ML to surface the best suggestions for particular users. With every click that the user makes, their website experience becomes more and more personalized. ML and AI are leading to significant advancements in medicine as models trained to analyze images can identify various abnormalities to a high degree of accuracy. These are just a few examples. We'll explore many more in upcoming modules, along with the security, privacy, compliance, and ethical implications of leveraging data.




Using a new built-in security model
One aspect of digital transformation that permeates all others is security. Security in the Cloud requires new ways of thinking. Traditionally, IT security models focused on keeping threats out. They built an on-premises perimeter that individuals required access to in order to gain entry. That model works when all hardware and systems were controlled and managed centrally and employees came into the office to do their work. Now, employees want to create, share, and access information virtually. In an increasingly global workforce, businesses need to grant access to applications and relevant data with a high degree of security. Businesses can now do this with security built in when moving some or all of their data and infrastructure to the Cloud. In the Cloud, the best practice for security is called a Shared Responsibility Security Model. In this model, the Cloud provider is responsible for the physical infrastructure like the undersea cables, data centers, the personnel to manage the hardware and software, and businesses are responsible for controlling data and resource access. This means that businesses need to think carefully about appropriate governance and policies for granting and restricting access to information and applications. Compliance with regional regulations is also part of security and governance. These regulations govern where data is stored and how it's managed. Security in the Cloud is multifaceted and complex, and we'll cover it in a lot more detail in another course, Understanding Google Cloud Security and Operations. These are just a few of the security concerns that any organization must take into account as they undergo digital transformation. Up to this point in the module, we examined key business and technical challenges that organizations face as they undergo digital transformation. These challenges include culture change to encourage innovation, updating IT infrastructure, modernizing business platforms and applications, and capturing, storing, and leveraging data, and finally adopting a built-In security model. These challenges are complex, and in many cases, mission-critical for businesses to overcome in the Cloud era.




2.3 Google Cloud solutions

Google Cloud solutions for digital transformation
Google serves over one billion users worldwide across search, Gmail and other applications. Google takes what it has learned from serving billions of users and creates Google Cloud products and solutions available to organizations around the world. Now customers can build their own applications and manage their own workloads on the same infrastructure that Google Cloud runs to achieve their mission and serve their users. We've grouped the products and services into solution pillars to match many organizations transformation journey. In fact, these groupings are based on how different types of customers have already used and benefited from Google Cloud solutions. The solution pillars are infrastructure modernization, business applications platform portfolio, application modernization, database and storage solutions, smart analytics, artificial intelligence, and security. A common first step for digital transformation is moving parts of a traditional IT infrastructure to the Cloud. The goal would be to make modifications with minimal impact to end users or customers, learn from the updates iteratively and ultimately meet the demands of the Cloud era. Google Cloud and its partners offer flexible infrastructure modernization approaches from re-hosting customer's existing IT to re-platforming. This means using new platforms and applications to enhance what you can do. When organizations have moved some or all of their workloads to Google Cloud, they can then leverage the innovation built into Google Cloud technology to create new business value. We cover more details about the specific products and services in another course, the value of infrastructure and application modernization with Google Cloud. Now another challenge businesses face is modernizing their business platforms to enable better information flows and more secure access to systems and applications. With Google Cloud Business Application platforms portfolio, organizations can securely unlock their data with APIs, automating processes and creating applications across Clouds and on premises without coding. Tools in this pillar, such as Apache, API Management and Cloud Endpoints, build and automate business workflows while migrating and modernizing apps as they move to the Cloud or between Clouds. Businesses can better serve their users through application modernization. The tools within this pillar help businesses develop and run applications anywhere. Businesses can both modernize legacy apps and build new ones, which helps them achieve higher return on investment and innovate faster. Let's look at data and digital transformation. Google Cloud database and storage solutions include tools that help businesses migrate and manage enterprise data with security, reliability, high availability, and fully managed data services. Examples include Cloud Spanner, Cloud SQL, and Fire store. The smart analytics portfolio helps businesses generate instant insights from data at any scale with a serverless fully managed analytics platform. BigQuery is an industry leading example of a serverless data warehouse solution. Looker is a business intelligence platform that provides a unified service to access the truest, most up to date version of your company's data. We cover more on these and other tools in another course, innovating with data and Google Cloud. Google Cloud, artificial intelligence tools are built to enhance innovation and improve productivity by integrating seamlessly into a company's existing workflow and products. Google Cloud's comprehensive security solutions cover all aspects of protecting your business in this digital era. In fact, businesses can detect, investigate, and protect themselves against online threats before attacks result in damage or loss. These solutions also reduce the time it takes to identify threats. It might be difficult to determine what solutions you need and how to prioritize your cloud adoption challenges. To help organizations optimize their cloud adoption Google Cloud has developed the Google Cloud Adoption Framework. This best practice guide provides a framework to assess where an organization is in its journey and what it should do next. 
Refer to the linked reading or the direct link to the website for more details. Cloud.google.com/adoption-framework.




3.1 Overview
In the last module, we explored business and technical challenges that organizations face on their Cloud adoption journey. In this module, we'll look at a third challenge, culture change. At Google, we believe an organization's culture plays a key role in their ability to embrace change and adapt with new technologies. In this module, I'll start by exploring six focus areas that Google thinks are vital to transforming culture with an emphasis on culture of innovation. Next, I'll explore three simple rules that help organizations scale the innovation mindset. Then I'll apply the three rules in a real-world example from the banking industry. 




3.2 Culture Transformation

Focus areas for culture transformation
When you see Cloud as a tool to do things the way you've always done them, you risk vanishing into irrelevance. Using Cloud to do new transformative things means embracing wholesale change. This change may involve radically rethinking business practices, structures, and even business models so you can better serve your customers globally. Since forming in 1998, Google has grown from a few guys in a garage to an international organization with over 100,000 employees worldwide. Along the way, we've done a lot of thinking about how to maintain an innovation mindset, the same mindset that enabled Google's founders to build the Google search engine in the first place. We've also spent a lot of time helping other companies embrace and nurture an innovation mindset and learn from their experiences too. We've categorized the learnings by six focus areas that contribute to the successful culture transformation. They are foundational to creating a fast moving, customer-centric and future-proof business that optimizes its use of Cloud technology. These focus areas are talent, environment, structure, strategy, empowerment, and innovation. We need an entirely separate course to cover the details of each focus area. For this course, I'm going to briefly present each area before focusing on innovation and how it relates to digital transformation. Talent refers to a holistic view of the people that make up an organization and contribute to innovation. It covers the entire life cycle from attracting, to hiring, to nurturing, to retaining, to celebrating, and growing the talent. The ability of people to thrive in an organization, especially during major changes, is connected to the work environment. Environment, our next focus area, means more than just a workspace. Every program, every perk or service should be designed to enable a culture of innovation and efficiency and ultimately lead to job satisfaction and overall well-being. That brings me to the next focus area, structure. Structure is a blueprint for how certain programs and tasks are grouped and how people managing them are led toward a common goal. Essentially, structure is how a business organizes itself. For example, how an organization establishes its hierarchy and management levels, and forms teams, and how people access information are all part of an organization structure. Strategy is how you align people to your organization's purpose or mission. It is the direction you set, how you measure progress, and how you adapt to new information to achieve your vision. Next is empowerment. Empowerment means enabling employees by giving them access to relevant information and encouraging them to use it to take initiative to solve problems and improve the business. Certain degrees of autonomy, independence, and responsibility can increase motivation, which is central to creating a culture of innovation. Lastly, innovation is central to embracing new technology. So let's look at this in more detail. Innovation, at its core, is about doing something in a surprising new way or discovering something entirely new that adds value. Whether you're rethinking an existing process or creating a totally new product, innovation involves creativity and ingenuity. Creating a culture where people can innovate is foundational to embracing meaningful change, adapting to and optimizing new technologies, and most critically, maintaining a competitive advantage in a fast moving world. However, innovation can't be owned or ordained, but you can create the environment and the right conditions for innovation to evolve organically. The fuel for innovation is a balance between freedom and constraint. At Google, we strive to give employees the right amount of creative freedom and psychological safety so innovative ideas can scale. Google typically follows three rules to foster and scale a culture of innovation.




3.3 Three Simple Rules

Scale the innovation mindset
In the previous video, I mentioned that Google has established three simple rules that govern its day to day business practice and help to nurture and skill a culture of innovation. They are; focus on the user or the customer, think 10x or generate big ideas, and finally launch and iterate, which is often referred to as continuous learning. These principles weren't created by Google. Every company focuses on their customers. Every company expects its employees to be bold and to generate big ideas. Where Google differs, however, is in the way it implements these principles to scale the innovation mindset. We'll look at each one by one. Let's start with focus on the user. Focus on the user as a business practice may sound common. How often have you heard customer-first? But for us at Google, this focus has two dimensions. First, users aren't limited to paying customers or people outside our business. Our employees are also our users. Next is user expectation. You need to clearly understand user expectation to think about how you can add value for them. This is because with the digital age, everyone has become connected globally via multiple devices. Everyone has acquired the same expectations when engaging with companies. Here's an example of what I mean by focusing on your customers and their expectations. When airlines first started offering WiFi on-board their planes, it seemed new and revolutionary and it differentiated in airline companies offering. Now, it's expected. Notice that as soon as your customers become exposed to something new that makes their lives easier, it doesn't take long for it to become an expectation. User expectation, though, can mean a variety of things. To help you narrow down the scope of users expectations, consider the following focus areas; access, engagement, customization, and communication. For each focus area, ask yourself, what is the user or customer expect. By answering the question, you'll make important discoveries about where to invest your efforts. Let me give you a few examples. When it comes to access, users expect faster and easier services with always on capabilities that can be accessed anywhere. In terms of engagement, users are looking for sources of valued content. They expect up-to-date reliable content from multiple fields of expertise. From this, you can then learn that engaging multiple fields of expertise in the process of product development is also crucial for your business success. Next, when it comes to customization, users expect that a product or service seamlessly adapts to their individual needs and preferences. Finally, users expect to be able to communicate with service providers through a two-way feedback channel. This means that the company also engages in conversation. The same is true for your employees. When assessing what they expect when it comes to communication, you'll discover that they expect their contributions to matter and that their input has a positive impact on the evolution of a product or the company. To meet their expectations, two-way dialogue is needed between teams and between employees and their leadership. Take a moment to think, who is your user? With innovation in mind, is there room to drastically improve or reinvent your products or services by examining your user's expectations? Now, at first, this level of focus on the user might seem like a bad business decision. What about ensuring that the business is making a profit? Believe it or not, there is a different way to look at the problem. I'll give you an example. One of Google's primary business models is built around ad sales. In the past, when users went to Google's site to search for information, they would see company bought ad space on either side of the search window. By applying the first rule, focus on the user, Google decided to update its user interface design. This involved removing the ads and showing more information for some search results in order to provide a better user experience. At the time, 87 percent of Google's revenue came from ad space sales. This might have seemed like an irresponsible decision. But Google focused on the user and made the changes anyway because its mission isn't to sell ad space but to organize the world's information and make it universally accessible and useful. Coupled with the capabilities of the Cloud, Google actually discovered that an improvement in the user experience didn't negatively affect its revenues. In fact, users have more insight now than they ever did before when they type in just a few letters. Let me give you an example. Last Sunday, I was deciding between pasta and sushi for dinner. I can make pasta, but not sushi, and I was really craving sushi. At 8:40 PM, I used Google Search and typed in fuki, the name of a local sushi restaurant. I didn't type the word sushi because geolocation knows I live in Palo Alto and so the search retrieves fuki sushi in Palo Alto. Immediately, I have a tremendous amount of information. Photos of the restaurant and the restaurant's hours. I see its location on maps, which shows how long it will take to get there. I see in orange font that the restaurant closes soon. A bar chart generated via Google Map's aggregated user data indicates how busy the restaurant is and tells me that the average wait time is 30 minutes. At the time of my search, it was 8:40 PM and the restaurant closed at 9:00. So I knew I wasn't going to get sushi. This is amazing. I type in four letters into the Google search bar, and within moments, I know I'm cooking pasta. Users find this functionality useful so they keep coming back. Eliminating ads for some searches and using sponsored links instead turned out to be very profitable for Google. Focusing on the user can help any organization leverage new technologies as they undergo a digital transformation. Focus on the user is good practice to help you remember what ultimately matters most, achieving your mission, why you exist, not how you operate. 




Think 10X
10x thinking is about generating big ideas. It's about transformation over improvement and using technology to achieve that transformation. Improvement projects help make things better by perhaps 10 percent, and there certainly is room for improvement projects in every organization. But they will not help you scale the innovation mindset. One great example of a 10x thinker was Henry Ford, an early innovator who brought the automobile to mass market in America. He said, "If you would ask people what they wanted, they would have said faster horses.'' Imagine how long it would have taken for the world to experience motor vehicles if everyone thought to only improve what they already had. This is the 10 percent mindset. Let's look at a Google example. In 2008, Google didn't have a presence in Lahore, Pakistan, which meant that a map shown on Google Maps was very limited. Improvement thinking would have led Google to make only 10 percent additions to the map content over time. But instead, Google followed the 10x principle and reframed the problem. How can we use technology to improve the information gaps in our map 10 times faster? By setting this challenge, Google was able to create a software program called Map Maker that asked its users to map their environment, empowering the user to provide Google with the missing data. The University of Lahore then launched a contest for students to map their journey from home to school using Map Maker. In just two years, with the support of users, Google was able to create a highly accurate map of Lahore. This data became vital in 2010 when there were terrible floods, four million people had to be relocated. A post-flood study found that 400,000 lives would have been lost without access to maps and the work of those students. Now, just imagine how long it would have taken to build an accurate map if we had applied the 10 percent thinking. Would it have even been possible? 10x thinking leads to solutions that are simple, empowering, and deeply transformative. Just like focusing on the user, thinking 10x helps organizations achieve their mission in new ways and differentiate their offerings from competitors. 




Launch and iterate
Everything you've learned so far comes together with the final rule, launch and iterate, which is often referred to as continuous learning. This is a break and burn and fail fast idea that you want to encourage in your company culture. It gives your employees the freedom to innovate and enables them to apply 10x thinking. What does it mean to launch and iterate? Launch and iterate is both a mindset and a practice where instead of starting off with a perfect solution, you figure it out through experimentation. Launch and iterate says, ''Try, learn from the output, and then try again.'' You can apply the launch and iterate rule to your own work by asking yourself, does my project or initiative support my why? Am I applying 10x thinking? How am I to use technology to reframe the problem or find a transformative solution instead of minor improvements? When you have the answers, start experimenting and building. Seek feedback quickly, and you might fail the first time, but if you do fail fast, instead of wasting your time, perfecting your idea. Why? Because in failure, you learn quickly. This is the process of innovation. There's one caveat to this rule. For everyone to launch and iterate freely, it's important to create a culture of psychological safety. Organizational Behavioral Scientist Amy Edmondson of Harvard first introduced the concept of team psychological safety. She defined it as a shared belief held by members of a team that the team is safe for interpersonal risk taking. There's a strong correlation between innovative teams and high psychological safety rates; teammates feel safe to ask questions, take risks, challenge each other and build on each other's ideas. They feel confident that no one on the team will embarrass or punish anyone else for admitting a mistake, asking a question, or offering a new idea. When people feel psychologically safe and begin applying the launch and iterate rule into their day-to-day work, the result is the prototyping effect. The more ideas you try, the more you learn, and the more you'll eventually succeed. Continuous learning and the ability to adapt based on that learning is critical as you adopt cloud technology for your organization. One high-profile example of this is Google Glass. This image demonstrates the evolution of Google Glass. The brainstorming discussion probably began with someone asking, ''If we can get information to people at their fingertips with a smartphone, how might we enable them to retrieve or receive information hands-free?'' From this, the prototype of the digital glasses emerged. It went through six iterations before reaching its most recent version. Very early, it became apparent that there were barriers to adoption for the mass market, cost being one of them. Another was social expectation, people felt awkward walking around with them. But Google employees maintain their innovation mindset and their ongoing curiosity, which led to an interesting discovery. Although the mass market wasn't ready for Google Glass, which was an apparent failure at the time, there was significant enterprise demand for it. Imagine you were a surgeon or handler in a warehouse or a repair mechanic for large industrial equipment, hands-free access to information that helps you do your job better is highly valuable, and in those cases, the cost is less of an obstacle. In July 2017, Google launched the new enterprise edition of Google Glass. The initial plan to create and mass market digital glasses did not succeed, but teams learned from each iteration evolving and adapting at each stage and responding to customer user feedback, which led to new successes. Remember, ideas don't have to be limited to hardware or service products. This way of thinking can also be used for any employee-customer or employee and employee interaction. You might be thinking, ''Hey, Saman, that's all great, but I can't always apply these in my organization.'' I agree. Although these principles help to nurture a culture of innovation, they may not always be applicable to every situation and every case. Still, I want to challenge you to try and apply them even at a small scale. I'll demonstrate this with an example from the banking industry next.




3.4 Example

Innovation example: Banking industry
Let's look at what cultivating and scaling an innovation mindset might look like in a real world situation. This is Jane, a banker. Part of her job involves cold calling customers and offering them a new service or product. She knows, though, that customers are rarely receptive to these types of calls. Jane's manager chooses a traditional approach and preassigns Jane a list of people to call along with talking points to sell insurance. What do you think is wrong with this approach? Well, think about it this way, what's Jane's why in this scenario? Is it to sell insurance? No, that's a specific task, not a mission statement. Does selling insurance meet the three simple rules for day-to-day business? Is it focusing on the user? No. Is it 10x thinking? No. Is Jane, as an employee, empowered to innovate in this case? Probably not. So let's look at Jane's situation in a different way and apply the three rules. What is her why? It's to enable people to live their lives feeling financially secure and optimally prepared for life's inherent ups and downs. Now we have a why that's a purpose, a mission. Next, we employ the first rule. Focus on the user. Imagine Fred is one of Jane's customers. He has two young children and is the type of person to plan ahead. He's starting to think about his kids' future and their education because he realizes it's going to be very costly. At the most basic level, Fred might need to know how much he needs to save and for how many years so he has sufficient funding to send both kids off to college. He might also expect that his financial institution can offer him personalized options to achieve this without disruption to his lifestyle. So Jane would focus her call with Fred on A, discovering his intent or needs, not the direct selling of insurance, and B, helping Fred with investment options. Now, how can Jane apply 10x thinking to our customer situation to find solutions that are simple, empowering, and deeply transformative? How can she anticipate Fred's needs and behavior so that she can propose solutions that will help him live life to the fullest and ensure that he is financially prepared and secure for this stage of his life? Jane and her team can apply 10x thinking to create breakthrough experiences to serve a customer with technology. Now, ask yourself, how might the capabilities of the Cloud help Jane achieve this? Well, imagine she has a dashboard that synthesizes multiple sources of data about Fred. This would give Jane, A, insight about his current situation, B, information to help predict his intent, and C, a list of pre-populated recommendations that she could discuss with Fred. The capabilities of the Cloud equipped Jane to have a much more focused and meaningful conversation with Fred. Now she can call Fred and say, "We think now would be a good time to talk about your children and their future education. What are your thoughts on this and how are you currently preparing?" She might follow up with, "We have some personalized recommendations for you. One of our experts is available to discuss your goals and investment options this week." Imagine if everyone across Jane's organization embraced this innovation mindset and employed Cloud technology to find innovative solutions. It would be an important step in the organization's overall digital transformation.




Try it Yourself
Embracing cloud technology and cultivating an innovative mindset are not limited to elite data scientists or company leaders. You can cultivate this mindset in your role, in your team, and across your organization, no matter where you are. Grab a piece of paper and write down your answers to the following four questions. First, what is your why? Write down in one sentence, what your mission statement is, then take this one step further and write down your team's mission and finally, your organization's mission. Next, who is your user? What are some ways you're focusing on their expectations and meeting those expectations in your day-to-day activities? When you know who your users are, ask yourself, how might I use new technology to serve them 10 times better? Finally, what would it take to launch your first idea? How much time do you spend perfecting an idea before sharing a first draft and iterating it with feedback? Are there ways you can launch and iterate more often? Let's look at a sample set of answers to these questions with a recent use case. Jorge is on the IT support team at a multi region enterprise. His why is to ensure that all employees are able to do their best work with the right access to the right digital tools. This means that Jorge's users are not the direct customer, instead they are the employees who work in the company. They expect to be able to easily connect with the support team at any given time, get a resolution to their issues without disruption to their work, and provide feedback for improved services. When 2020 brought us dramatic changes across the world and forced many employees to work remotely, Jorge needed to provide support at scale. He needed a solution that was 10 times better than what he and his team used to do when everyone could simply walk up to a support desk or file a ticket. He mobilized a team of 10 and set up a site called TechShop that was accessible by anyone logged into their corporate laptop. They added several new services to enable employees to reach them and to resolve their issues, including screen share via Google Meet. As they gathered feedback from users, they were able to improve the resolution time. In fact, based on the data collected about the most common issues, they were able to implement a repeatable workflow that automatically resolved users issues up front upon completing a digital questionnaire. Let's look at another example. Shreya is part of the logistics team for a retail company. Her company embrace cloud technology and digital transformation, so they now serve many customers via their online store. She quickly shifted her own role from managing in-store inventory to coordinating fleet management, including driver schedules, delivery, prioritization and overall efficiency. Her why is to get customers what they need, when they need it. Before the pandemic, Shreya met her customers' needs directly in the stores, and now she caters to her customers across the country through large scale delivery. Her users are, therefore, both the end customer and the drivers who are making this mission possible. As her team adapted their business model amid a pandemic that prompted the need for a major technological shift to meet greater online demand from customers, Shreya relied heavily on the launch and iterate principle to do her part. She began identifying innovative opportunities by collecting data from the end-to-end delivery process, from wait times, to loading the delivery trucks, to time and traffic, to delivery. Based on early data, she was able to find more efficient methods for loading boxes. But that's not all, Shreya and her team compiled all of their data sources with cloud technology to predict customer demand per specific day of the week. This insight drastically improved her business operations overall. Now, these are just two examples of how embracing the innovation mindset can unlock digital transformation in any role. 




Course Summary
Congratulations for making it this far into the course. We covered a lot. So let's recap the key points. Cloud technology allows businesses to combine powerful computer technology with vast amounts of data to create new value. As access to cloud computing resources has become more globally available, early adopters have been able to leverage it to disrupt industries and redefine customer experiences. You learned that customers now expect fresh, relevant, and easily accessible content almost instantly. They also expect real time services that are always running and available from anywhere in the world. Every company now faces the decision to embrace Cloud and thrive or vanish into irrelevance. You learned that organizations face multiple business and technical challenges no matter where they are in their digital transformation journey. These key challenges are fostering innovation through culture change, updating IT infrastructure, modernizing business platforms and applications, seamlessly capturing, storing, and leveraging data for better decision making, and finally, adopting a built-in security model. You also learned about Google Cloud solution pillars such as business applications, data storage, and smart analytics. These give organizations structured pathways for overcoming these challenges and creating new business value. In addition to the technology, another focus area we covered in more detail was scaling an innovation mindset through culture. Now, we recognize that what you learned here is just the beginning. We encourage you to continue to learn about unlocking the value of data by completing the course, Innovating with Data and Google Cloud. Find out more about the common pathways to digital transformation and fundamental Google Cloud products you should know about in The Value of Infrastructure and Application Modernization with Google Cloud. And finally, take Understanding Google Cloud Security and Operations to learn even more about cost management and monitoring in the Cloud. Check out the Google Cloud training catalog at cloud.google.com/training. And if you've signed up for the learning path, be sure to complete all four courses to get credit. And that's it for this course. Now that you know the basics, you can continue your learning and begin exploring Cloud-related initiatives in your organization and with your peers.





































#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#

Innovating with Data and Google Cloud


1.1 Overview
Data is a term that's used a lot in today's business world, and there's a good reason for that. Capturing, managing, and leveraging data is central to redefining customer experience and creating new value in almost every industry. In this module, I'll start with the basics. What is data, and what's its role in the digital transformation of your business? Then I'll discuss how you can leverage data in your organization. Next, I'll break down the types of data, and finally, I'll go through some important data considerations for every business using data in the cloud.




1.2 Data and Digital Transformation

The role of data in digital transformation
Let's start by asking a simple question: what is data? Data is any information that is useful to an organization. Imagine numbers on a spreadsheet or text in an email. These are both examples of data. Other examples include audio or video recordings, images, and even just ideas in employees' heads. Businesses now have access to data like never before. This includes internal information, data from inside your organization, and external information, customer and industry data. For example, as organizations have digitized their operations, all kinds of business data has become available, such as financial information, logistics data, production output, and quality reports. Businesses also have access to new kinds of data about their customers. Consider digital interactions such as the length of time a user spends on a web page or reaction to a social media post. These are totally new and very rich sources of information about customer behavior. The Internet has also increased access to external data, such as industry benchmarking reports. Capturing and leveraging this data to unlock business value is central to digital transformation. Large enterprises with traditional IT infrastructures face several limitations in leveraging the value of data. These limitations include processing volumes and varieties of new data, either at regular time intervals known as batch or in real time. Finding cost-effective solutions for setting up and maintaining data centers. Scaling resource capacity up or down, and regulating their capacity globally, especially during peak demand times throughout the year. Accessing historical data that is often stored in different formats and on different platforms. Deriving insights from historical and new data in time and cost-effective ways. Public cloud services like Google Cloud offer organizations economies of scale, rapid elasticity, and automation where there was manual overhead. They allow organizations to bring together data points and platforms fragmented across their whole ecosystem. In particular, the cloud provides data solutions that were once almost impossible. Businesses can now consume, store, and process terabytes of data in real time and run queries at its request to retrieve and use data instantly. Resources are now distributed across a global network. This means that multiple data centers can create resilience against data loss or service disruption, but without any extra overhead for businesses. And data can be combined, analyzed, and served to business teams quickly and cost-effectively. For the first time in many businesses, this means data insight is highly accurate and accessible across the business, and now an enabler of transformation. Let's look at some examples of organizations that have transformed their business by unlocking the value of data.

Budget airlines don't provide food as part of their service. Instead, they charge customers for meals if they want that. This may seem like a cost-effective solution, but it's often difficult to estimate the number of meals required onboard. If the airline overestimates the number of meals required, they risk wasting food and losing revenue. But if they underestimate the number of meals needed, they risk selling out of food, providing poor customer service, and losing potential revenue. One budget airline in Asia embraced digital transformation and reimagined how they could solve this problem using data. First, they identified factors to help estimate stock such as the size of the plane and the number of passengers, but they soon discovered that estimates based on these factors were not highly accurate. This meant that they had to think about their data differently. So they analyzed additional information such as destination, time of day, and flight connections before and after their journey. Using this information, they uncovered actionable insights. For example, they learned that of the total vegetarian and non-vegetarian meals required on each flight, flights to and from India required 73% more vegetarian meals. With these new insights, the airline was able to predict the amount of meals required more accurately. As a result, they provided a more positive customer experience and improved the profitability of their food service. Let's take a look at another example. Traditional retailers have access to a range of data about their stores, including stock levels, items purchase, and average spend per customer. However, they've never been able to capture information about more nuanced in-store customer behavior. One video security company noticed this problem in the retail sector and reconsidered how they could use their existing technology and data to overcome it. Traditionally, security monitoring systems were used for one main purpose: to detect criminal behavior in stores. But what if they could reimagine the purpose of this technology? By using cloud computing to mine data from video cameras and devices, this company was able to generate insights on customer retail footpath, sentiment, and dwell time. This means that businesses can now correlate data on shopper behaviors in the store to improve safety, operational efficiency, and top-line growth.

Manufacturing is another great example of an industry that is using data and embracing digital transformation. Companies and high-speed manufacturing industries such as pharmaceuticals, food and beverage, and consumer packaged goods require continuous production. They can't afford any downtime because that can significantly impact revenue, customer experience, and product quality. In these always on-manufacturing environments, maintaining production health is key. One technology company helps businesses perform vital monitoring of their manufacturing and production lines. They do this by combining IoT, Internet of things devices, with manufacturing and analytics. With cloud computing, they analyze historical data and live information generated by sensors to assess machine health, predict maintenance, and ensure that production lines are always running. We'll talk more about IoT in the next module. These are just a few examples of how cloud technology can unlock new value by reimagining data. No matter where you are in your company, you too can leverage data to solve challenges.




Leveraging data in your organization
So far you've learned that leveraging data is a critical part of digital transformation. And no matter what your role is, you can unlock new business value by thinking about data in new ways. A helpful starting point is to identify and map your data. Let me explain. A data map is a chart of all the data used in end-to-end business processes. For example, imagine that you own a chain of apparel retail stores. What might you include in your data map? A customer purchases an item in one of your stores. That's a data point. If you aggregate that with other purchases across stores in each region, you have a type of data: transactions. We call this a dataset. Another dataset might be item returns. Another is footfall in your stores. You may have noticed that all of these datasets are about your users. User data is therefore your first data bucket. This category contains all data from customers who use or purchase your services and products. Now let's think about data that is more operational. For example, data about staffing levels in each store, stock delivery dates, overall sales performance of each store, and store staffing structure. That's, how many people are in fitting rooms versus at the cash register. These fall into the second bucket of data: corporate data. Corporate data includes data about the company such as sales patterns and operations. Can you think of more datasets that you'd have as the retail owner? A third category serves as the umbrella for user and corporate data. We'll call it industry data. Industry data is data found outside an individual organization that everyone in the sector needs to view or access to gain knowledge about a specific domain. This could include wider trends, purchasing patterns, and publicly available research papers. These three buckets make up your data map. As you add more datasets to each bucket, you'll eventually gain richer insights. But how can I sort these datasets further? Well, I can use a check mark to identify the datasets I currently have and a question mark to label the datasets I think I can get. Now your data is mapped out with a list of the different datasets you have or think you can get. How can you make your data actionable? Start playing with the intersections between your datasets. Take two or more datasets and ask yourself, "What insight could I gain if these datasets were combined?" For example, suppose you're a sales manager at a biomedical diagnostics company. You provide a range of diagnostic tools to hospitals and laboratories across the region. One of your datasets is sales of each product. You also have some datasets about the hospitals themselves, including specialties, location, patient turnover, and central laboratory facilities. In this case, by integrating how many products are sold at each hospital with all the other datasets you have for the hospital, you have a clear insight into what makes your ideal customer. Here's another example. If you took introduction to digital transformation with Google Cloud, you might remember Jane. She's a personal banker. What does her data ecosystem look like? Well, user datasets might include user demographics, user financial history, and previous user interactions. Corporate datasets might include sales by financial product, sales conversation call logs, and performance metrics of financial portfolios. Banking is a heavily regulated industry, so a lot of industry data is available, including industry benchmarks. Another set of industry data is stock performance and other investment trends. Jane, her colleagues, and her competitors at other banks all have access to this data. By integrating her user demographics data with product purchases, she can begin uncovering demographic indicators that are significant for predicting product purchases. Now it's your turn. Take a piece of paper and write user data, corporate data, and industry data. Think about what dataset you have in each. Then consider how different datasets can be combined to create valuable insights.




1.3 Data Types and Considerations

Understanding data types
We've talked a lot about different data formats throughout the course, such as images, audio files, and social media interactions, as well as tabular data like sales figures. Most importantly, we've uncovered how you can combine and leverage data in new ways to revolutionize business models and create new value. To better understand how to combine this data throughout your digital transformation, let's explore the two types of data and what they mean for businesses. We can categorize data in two main types: structured and unstructured. Structured data is highly organized. Examples include customer records consisting of names, addresses, credit card numbers, and other quantitative data. Structured data can be easily stored and managed in databases. By contrast, unstructured data has no organization and tends to be qualitative. Examples of unstructured data can include word processing documents, audio files, images, and videos. This data can be stored as objects. An object consists of the data itself, a variable amount of metadata, and a globally unique identifier.

Some unstructured data can be stored in a format called a BLOB. This stands for Binary Large Object. Images, audio, and multimedia files can all be stored as BLOBs. Organizations rely on both structured and unstructured data to gain insight and make intelligent decisions. However, unstructured data has historically been very difficult to analyze. Cloud technology changes this. With the right cloud tools, businesses can extract value from unstructured data by using an application programming interface, or API, to create structure. An API is a set of functions that integrate different platforms with different types of data so that new insights can be uncovered. Let's look at an example of a used car dealership that was able to leverage both structured and unstructured data.

Whenever customers brought cars into the dealership, the agents had to manually upload and label photos of each car and then set a price depending on the model and condition. This process took them, on average, 20 minutes per car. In this case, how could the car dealership use automation to make this process more efficient? Well, they decided to develop a custom train machine learning model, a topic I'll cover more in detail in an upcoming module. To combine unstructured data, the photographs, with structured data, they used car prices. They could then use this combined data to predict the price value of a vehicle. With this new approach, the overall process to photograph and evaluate a car has now dropped from 20 minutes to 2 to 3 minutes per car. That's a 95% reduction in time and a massive improvement in overall service. Another example is Bloomberg. Bloomberg aggregates data-driven news, global insights, and expert analysis from over 2,700 journalists in more than 120 countries. They also make their content, their structured and unstructured data, available in multiple languages worldwide. This is inevitably a big challenge. Their customers expect up-to-date and accurate news that's accessible in their specific languages. So how do you translate the data you're receiving and then localize it into your global audiences, all in real time? Well, Bloomberg uses Google Translate API, which enables them to make financial news, global insights available to their customers in as many as 92 languages. Using an API to translate content into multiple languages is a common way that businesses apply the power of the cloud to unstructured data, creating new value while saving time and costs. More importantly, they can reach more customers and provide a better, more personalized service. Understanding structured and unstructured data can help you define what's possible with the data solutions you have. We'll explore this in the next module when we cover data consolidation and analytics. But first, any conversation about data needs to include a reference to security, privacy, compliance, and ethics. We'll cover that in the next section.




Important data considerations
Capturing, storing, and analyzing vast amount of data is key to adopting Cloud technology. But handling this volume and diversity of data comes with its own ethical considerations and requires alternative ways of thinking about security. Google believes that capturing and managing data demands responsibility and accountability. Not all information that can be captured should be captured. In other words, businesses are accountable for making responsible decisions about which data they collect, store, and analyze. This also extends the data that businesses already own. In this case, it's essential to examine who has access to the data and how they'll be using it. First, consider the source of the data, how it's being collected, and where it's stored. If it's personal or sensitive data about a customer or an employee, it needs to be securely collected, encrypted when stored in the Cloud, and protected from external threats. Additionally, only a subset of users should be granted permission to view or access the private data. Data security and privacy becomes more complex in a global economy. Regional or industry-specific regulations often guide data policies. Google Cloud offers a range of solutions and best practice resources that companies can leverage. Another consideration is whether all the data is relevant and appropriate. Let me explain where this can be particularly important. Suppose, for instance, you want to use thousands of lung X-ray images to train an ML model to automatically identify tumor markings in new patient X-rays. What you need are the X-ray images. This is the relevant data. What's not relevant is patient's personal data. You need to ensure that any source data about individuals such as names or addresses is omitted or redacted. There's also some information that is not personally identifiable and should still not be included in the modeling for ethical reasons. A good example of this is whether or not individuals have health insurance. This is not relevant for educating the model to identify tumors. And if the data is included, the solution could be discriminatory. These ethical and privacy considerations are particularly complex when you're working with unstructured data. For example, a customer support team that resolves hundreds of customer's issues a day via mail might want to use an automated tool to find patterns in the email passages and develop targeted solutions. It's true that emails contain valuable data that can be mined to solve this challenge, but it's essential to be conscious of protecting customer privacy at the same time. Ethical and fair considerations are particularly important and applicable when you work with artificial intelligence, AI, and machine learning. We'll cover ML and important factors to consider in a later module. For now, I want you to remember that human bias can influence the way datasets are collected, combined, and used. Because of this, it's always important to include strategies to remove unconscious biases as you start to leverage data to build new business value. In this module, you learned the importance of data in digital transformation. Unlocking the value of data enables a business to both rethink how they serve their customers and reimagine how they operate. Ultimately, using data effectively enables any business, large or small, to better achieve its mission. Move on to the next module to learn about data consolidation and analytics. How you store and manage your data affects what you can do with it. So in the next module, we'll examine the challenges, solutions, and use cases for different data consolidation and storage systems on-premises or in the Cloud.




2.1 Overview
In the previous module, you learned that unlocking the value of data is central to digital transformation. Businesses that leverage data effectively can improve efficiency and productivity, deliver fresh, personalized customer experiences, and create new business value. In particular, I discussed the different types of data that businesses can access, and how you can combine them to generate insights and take intelligent action. The way that data is collected, stored, and managed is foundational to what you can do with it. In this module, I'll start by considering where data is now and the benefits of migrating your data to the Cloud. Then I'll define key terms related to data storage, including database, data warehouse, and data lake. For each term, I'll cover some use cases in applicable Google Cloud Solutions. Finally, I'll close the module by exploring business intelligence solutions like Looker, which enable businesses to gain insight into their data. 




2.2 Data Migration

Migrating your data to the cloud
Think about your company or customer data. Where is it currently stored? Some of your data may already be stored in the Cloud. But for most large organizations, a vast amount of data is still stored on-premises or buried on individual computers. When you consider the value of your data, storing data on-premises in silos is like storing your money in a mattress. And if there's data somewhere in your organization that isn't being tracked, it's vulnerable to user attacks and it's unproductive. To get the most value from your data, you need to know what you have, find it easily, and be able to use it while keeping it secure from external threats. This means that how data is stored and processed is critical to business success. If you're storing your data on-premises, you'll need to start thinking about taking some or all of it to the bank, or in other words, to the Cloud. It'll provide a greater return on investment. How? I'll explain. When you store your data on-premises, you're responsible for the IT infrastructure that supports the collection, security, and processing of that data. You're also responsible for maintaining and expanding the capacity of your IT infrastructure. This can be expensive and time-consuming. It's also difficult to scale quickly. If you have limited storage space, then you risk missing opportunities to gain insights with new data. You also risk downtime. Imagine that you launch a mobile app and don't accurately predict demand. A lot more people download and use it than you expect. Your infrastructure isn't set up to absorb that much data that fast. As a result, the app freezes or crashes, resulting in dissatisfied users. With Cloud, you can rent space from public Cloud providers like Google Cloud. This means that your data storage and compute powers are elastic. It can scale up or down as the data you take increases or decreases. Think about our money analogy. Unlike your mattress, no matter how much your money grows, the bank can hold it. Another way that migrating data to the Cloud provides a better return on investment is the speed at which you can ingest and use data, in particular, the speed at which diverse formats of data can be analyzed and used. Let me explain. In an earlier module, we discussed how data analysis was traditionally retrospective. Business analysts would gather data from the past six months and run analytics to provide insight into what happened during those six months. The analysis could take a few days, maybe even a few weeks. Now businesses can ingest data in real-time. For example, a business can use Cloud technology to track customer behavior on its website. Each click, scroll, and back click is a data point. Think about the millions of users viewing a website across the globe at any given time. That's a lot of data. But website clicks are only one kind of data. With the right Cloud tools, businesses can unlock even more value with data. Now a business can combine user clicks with other data to understand user intent. For example, if a user makes regular purchases from your online store, one data point, and provides a five star rating a few days following each purchase, another data point, then those data points can be combined to determine intent. The way you store this data provides the foundation for how you use it. In the upcoming videos, I'll define database and data warehouse as the two main storage formats for structured and semi-structured data. Then I'll explain how businesses typically use them and offer some Google Cloud data storage solutions.




2.3 Data Solutions

Cloud databases
In the last video, I mentioned that how data is stored is central to being able to use it. There are many solutions for data storage. One format is a database. A database is an organized collection of data generally stored in tables and accessed electronically from a computer system. Companies typically use a database to keep track of their basic online transactions, provide information that will help the company run its business efficiently, or help managers and employees make better decisions. For example, a hotel booking site would use a database for their customer transactions. If a person books a room for a night, that data is captured in the database, and the room availability is updated in real time on all customer channels. Another example is online banking. When someone transfers money from one account to another using their mobile app, that figure is updated in the bank's database in real time and the user is able to see the most up-to-date account balance.

Data integrity and scale are two priorities for businesses that use databases. Data integrity or transactional integrity refers to the accuracy and consistency of data stored in a database. Data integrity is achieved by implementing a set of rules when a database is first designed, and through ongoing error-checking and validation routines as data is collected. Databases, therefore, also allow businesses to roll back transactions to see data history. In our banking example, suppose the customer goes to an ATM to check their account balance. It's not the same at what's displayed on the mobile app. In this case, the bank needs the ability to roll back the transactions to identify the source of the problem. Perhaps the ATM is broken, or perhaps the user didn't click the final transfer button on their app. This rollback integrity protects the bank from fraudulent claims and protects customers money. Another priority when using databases is scalability. Going back to our example, suppose the ATM belongs to a global bank that has a large customer base and processes high volumes of transactions every day. They need a database that can scale to meet that demand. Or think about our hotel booking example. Websites that process global holiday bookings have millions of transactions happening every day that require transactional integrity at scale.

Different cloud providers offer a range of database solutions. Let's look at a couple of the most common Google Cloud database services and their benefits. We'll start with Cloud SQL.

Cloud SQL is a fully-managed Relational Database Management Service, or RDBMS. It's easily integrates with existing applications and Google Cloud services like Google Kubernetes Engine and BigQuery and built on the performance innovation in Compute Engine. Cloud SQL is compatible with common database management systems and methodologies.

It offers security, availability, and durability, and storage scales automatically when enabled. This makes it easy for organizations to set up, maintain, manage, and administer databases in the Cloud. You might want to use Cloud SQL for databases that serve websites, for operational applications for e-commerce, and to feed into report and chart creation that informs business intelligence. Let's look at a specific customer example headquartered in Mumbai, India. Living Consumer Products runs two flagship products: a casual dating mobile app called iCrushiFlush and a contextual digital platform, CDP, that provides digital marketing services to clients.

By signing on to iCrushiFlush through Facebook, users provide details such as gender, location, and interest, as well as headshot images. iCrushiFlush stores this information in a database and displays it to other iCrushiFlush users through an algorithm depending on compatibility. Due to the large data volumes generated by iCrushiFlush and CDP, as well as the need to allocate scarce personnel and financial resources to business projects, Living Consumer Products decided to operate in the Cloud from the beginning. However, testing and early experiences with public cloud services didn't meet the company's cost requirements. In addition, they felt that availability and scalability were non-negotiables. And they couldn't afford downtime that would drive users away. Living Consumer Products migrated iCrushiFlush and CDP to Google Cloud. Now they're running these services on Compute Engine to provide compute power, Cloud Storage to provide unified cloud storage, and Cloud SQL to run its relational database. This allows Living Consumer Products to both store and retrieve large volumes of data such as user images in real time. We'll cover Cloud Storage in more detail in upcoming videos.

By handing off to Google the time-consuming tasks required to set up and run a database like applying patches and updates, managing backups, and configuring replications, you can save time and money, and keep your focus on building great applications. Cloud Spanner is another fully-managed database service, and it's designed for global scale. With Cloud Spanner, data is automatically and instantly copied across regions. This replication means that if one region goes offline, the organization's data can still be served from another region. It also means that queries always return consistent in ordered answers regardless of the region. For example, if someone in the London office updates information in the database, that update is immediately available for someone in the New York office. Consistency is critical to companies like Spotify. It provides users with music streaming services. If queries don't always reflect the latest change, this creates many challenges for the company. Spotify holds information about the objects it stores in the Cloud. This is known as metadata. Migrating their metadata storage to Cloud Spanner gave them strong listing consistency, so they know their queries will always reflect the latest data situation. Cloud Spanner provides strong consistency and massive scalability, which means that, for organizations, this is no longer a trade-off. Plus, it provides enterprise-grade security. This makes it ideal for organizations that want scalability for their databases, whether it's within a region or across the world. It's great for mission-critical online transaction processing, and because it's all managed, it dramatically reduces the operational overhead needed to keep the database online and serving traffic. Cloud SQL and Cloud Spanner are examples of databases that enable customers to manage high volumes of transactional data at speed and at scale. With Google Cloud databases, businesses can build and deploy faster, deliver transformative applications, and maintain portability and control of their data. Databases are one of the two main types of data storage systems in the Cloud. The second is data warehouses. Data warehouses allow businesses to unlock insights and take intelligent action. 




Cloud data warehouses
In the last video, we explored Cloud databases and how they enable businesses to ingest and use high volumes of transactional data. Let's now discuss the second type of data storage system in the Cloud: data warehouses. While databases store transactional data in an online fashion, data warehouses assemble data from multiple sources, including databases. Databases are built and optimized to enable ingesting large amounts of data from many different sources efficiently. However, data warehouses are built to enable rapid analysis of large and multidimensional datasets. For example, a dataset may capture every online sale every day of the week. But if you want to analyze those sales to identify trends or even to get the sum of total sales each day, you need a data warehouse. Perhaps you also want to identify sales trends by combining data such as new product rollout dates, marketing campaign language, and operational efficiency data. Think of the data warehouse as the central hub for all business data. Different types of data can be transformed and consolidated into the warehouse, so they're useful for analysis. In particular, a Cloud data warehouse allows businesses to consolidate data that is structured and semi-structured.

Remember that unstructured data tends to be unorganized and qualitative. In other words, it wouldn't fit in a spread sheet.

When combined with connector tools, data warehouses can transform unstructured data into semi-structured data that can be used for analysis. Let's look an example. Consider the online hotel booking example we examined in the previous video. Suppose they now want to do even more with their data. They want to use multiple types and sources of data to gain insights about hotel quality, and ultimately to improve their service to customers. They identify a list of possible data sources from their end-to-end customer journey such as: number of bookings by type of room, number of guests, and time of year; overall satisfaction during their stay and satisfaction with hotel staff, amenities, food, and check-in and check-out processes; customer posts on social media platforms by sentiment, location, or specific event at the hotel; and customer feedback and complaints captured via the website, mail, or in person at the customer service desk. All these different data types and formats are ingested and assembled into a data warehouse through different channels. The business can query the data warehouse quickly and at scale to derive meaningful insights. They can take their business's goal one step further and use the source data to build machine learning models to surface personalized hotel recommendations and tailored booking experiences for customers. We'll talk more about that in the next module.

For now, let's look at a Google Cloud leading data warehouse solution, BigQuery. BigQuery is a fully-managed data warehouse with downtime free upgrades and maintenance and seamless scaling. Most of all, BigQuery allows you to analyze petabytes of data using incredibly fast speeds and zero operational overhead. This means that as an organization, you can focus on analyzing your data to find meaningful insights instead of spending time and resources on maintenance. Most data warehouse providers link storage and compute together. So customers are charged for compute capacity, whether they're running a query or not. Importantly, BigQuery is serverless. This doesn't mean that there's no server. It means that resources such as compute power are automatically provisioned behind the scenes as needed to run your queries. So businesses do not pay for compute power unless they're actually running a query. Ocado is one of the world's largest online-only grocery retailer. It experienced significant growth in its early years of business. As a result, it began to find that its old databases just weren't fast enough to meet business demands. Now they've migrated to Google Cloud and use BigQuery.

They found that query results are delivered 80 times faster and at 30 percent less costs. Let's look at another example. Bueno is a Software as a Service, or SaaS company, that helps businesses meet their sustainability goals by improving building systems. Buildings are usually equipped with various networks that control and operate the facilities they contain. But air conditioning, lighting, and security systems often exist in their own silos with no link between them for communication. Bueno aims to bridge this gap by using technology to gather data and provide better transparency between the different systems, so that the building sector can use this information for fault detection, optimization, and business intelligence. Bueno has used Cloud technology from the beginning.

They decided to migrate to Google Cloud for a range of reasons, one being that they wanted to use existing data to better understand their customers and their business.

They do this using BigQuery. There are two other tools they use: Pub/Sub and Dataflow.

Pub/Sub is a service for real-time ingestion of data, whereas Dataflow is a service for large scale processing of data. Remember how I described data warehouses as the central hub for data to flow into? Well, these two different services, Pub/Sub and Dataflow, can work together to bring unstructured data into the Cloud and transform it into semi-structured data.

This transformed data can then be sent directly from Dataflow to BigQuery, where it becomes immediately available for analysis.

These tools enabled Bueno to unlock new insights about their customers, such as equipment and site level data, weather data, and even the customers' last visits. Now customers can use Bueno system to look for insights and discover new things that may require action being taken. Their customers can generate a work order for maintenance to be done and log the cost of the job. Bueno can then use that data to verify the value and accuracy of their analytics. Now that you understand the differences between data warehouses and databases and have learned about a few Google Cloud services, let's briefly explain how unstructured data can be stored in data lakes.




Cloud data lakes
In earlier videos, I covered databases and data warehouses, as types of data storage solutions. Another concept within data management solutions is data lakes, a data lake is a repository designed to store, process, and secure large amounts of structured, semi-structured and unstructured data. It can store vast amounts of data in its native format and process any variety of it ignoring size limits. The data warehouses' main purpose is to enable easy data analysis by transforming and consolidating data. Data lakes are inherently flexible, which is one of the biggest limitations of data warehouses. Analytics that are built solely on traditional data warehouses make it challenging to deal with data that doesn't conform to a well defined schema, because that data is often disregarded.

Data lakes are often made up of many different products, when determining which product to use for your data lake, you must consider the nature of the data being ingested. This flow chart shows the appropriate Google cloud storage products based on whether structured, semi-structured or unstructured data is being stored. For example, the best Google cloud product for storing unstructured data in a data lake would be cloud storage. Cloud storage is a service that enables you to store and serve binary large object or blob data, blobs are typically images, audio or other media objects. Cloud storage provides organizations with different options so they can tailor their object storage based on their access needs. In fact, some of the key benefits of Google cloud storage are, you can store unlimited data with no minimum amount required. Low latency, you can retrieve your data as often as you'd like, and you can access it from anywhere in the world.

Suppose for instance your organization is storing data that is frequently accessed from around the world. This might be data that serves website content or mobile applications or streaming videos. For this type of data, cloud storage offers multi-regional storage, it's ideal for serving content to users worldwide. We talked in the last video about Spotify, Spotify uses cloud storage to serve music to users around the world. Because cloud storage stores geographically dispersed copies of your data, your organization is less likely to lose its data in the case of a disaster. Regional storage is also offered by cloud storage, this is ideal when your organization wants to use the data locally. It gives you added throughput and performance by storing your data in the same region as your compute infrastructure. This is a great choice for internal use cases such as data analytics and machine learning jobs. For data that will be accessed less often, cloud storage offers nearline, coldline and archive storage classes. Nearline is best for data you don't expect to access more than once per month, such as multimedia file storage or online backups. Coldline is best for data that you plan to access at most once per 90 days or quarter.

Archive is best for data that you plan to access at most, once per year. Such as archive data or as a backup for disaster recovery.

Now, let's look at another example of cloud storage and use. In the financial industry, voice transcription has always been tricky, because it's jargon heavy and trading conversations are sensitive in nature. Cloud9 Technologies is a company that provides an innovative voice communication and analytics platform specifically built for the unique compliance and management demands of financial markets.

Their platform leverages Google cloud machine learning services, to automate voice to text transcription of trading conversations. The platform also uses cloud storage to house, the enormous quantities of information gathered. The data is encrypted by default and any sensitive information such as names is automatically reducted in the storage process.

All right so far we've covered three different types of data management systems, databases, data warehouses and data lakes. Each delivers value to businesses in different ways, enabling them to leverage data at scale. These systems and tools like Pub/Sub, Dataflow and BigQuery enable businesses to ingest and analyze data.

How is that data then served to the business to generate insights? I'll cover the answer in the next video.




Business intelligence solutions
Throughout this module, you've learned about databases, data warehouses, and data lakes as solutions to store and manage your data. Now let's look at business intelligence solutions that serve your data in the form of insights at scale. The challenge businesses often face is identifying the right business intelligence solution. Some solutions are too complex and not accessible by everyone outside the data engineering or data analysis teams. This means other teams have to put in requests and wait for answers, which defeats the purpose of gaining real-time insights. Other solutions let everyone in the business perform their own data analysis, but they can only perform their analysis with portions of the available data. This means that only a few people, or possibly no one, has a full view of the company's business data.

Looker is a Google Cloud business intelligence solution. Put simply, it's a data platform that sits on top of an analytics database and makes it simple to describe your data and define business metrics. Once you have a reliable source of truth for your business data, anyone on your team can analyze and explore it, ask and answer their own questions, create visualizations, and explore row-level details.

With everyone exploring this data individually, it's possible to discover greater insights and allow teams to share their findings easily with a simple link. And every answer becomes the inspiration to explore more. Let's look at an example. Gaming companies have to constantly innovate to remain relevant in a crowded market. Mobile and video gaming analytics provides insight into user behaviors. By investigating how users interact with their games, a business can develop a better understanding of their audience and use that to create more compelling games. For example, gaming analytics can be used by product managers, developers, and marketers to see which features are used most, discover levels or areas in the game where players are getting stuck, and identify player lifetime value. With this information, gaming companies can then create better, more targeted content for their players based on the needs, interests, and challenges of their users. Using Looker, gaming companies can combine marketing and behavioral data to acquire the right types of players for their games. This combination allows them to connect their revenue to marketing spend and determine which networks, campaigns, and creative strategies gain greater results. But some data like player retention and repeated gameplay are traditionally more difficult to analyze than other pieces of data. Looker leverages the power of data warehouses like BigQuery to make this data useful.

For example, it can standardize important metrics to create greater consistency and accuracy when other data analytics tools can only produce siloed results.

This is just one example of how an effective business intelligence solution can enable businesses to transform to better serve their customers. Now let's focus on how businesses can create new value with data.




3.1 Overview
In previous modules, we explored the critical role that data plays in digital transformation. I also covered the importance of how you can collect, store, and access data to enable effective data driven decisions. Volumes of data and the right cloud based tools are the foundation for using machine learning and artificial intelligence or ML and AI. To set the foundation for this module, I'll begin with the definition for ML and AI. Then I'll cover some important data quality considerations that influence the efficacy of machine learning models. Finally, I'll highlight several real world use cases in which customers have leveraged ML to radically transform their business.




3.2 Understanding Machine Learning

What is machine learning?
To understand machine learning, you have to start by thinking about data in your business. Do you have a dashboard that analysts view every day? Or maybe there's a report that your managers review each month. Both the dashboard and the reports are examples of backward looking data. They look at what happened in the past. Most data analysis in your organization is probably backward looking analysis of historical data to calculate metrics or identify trends. But to create value in your business you need to use that data to make decisions for future business. Let me give you an example, suppose Maya leads the business strategy and operations team for an international airline. She might be looking at historical annual reports to establish a trend in customer purchasing patterns. She'd probably use this data to forecast annual sales and operational costs. But there's nothing new or transformational about this decision making process. What if Maya could predict the satisfaction rate of each flight or predict customer complaints and get ahead of them. To do this effectively, she needs to access a lot more data, including number of passengers per flight, duration of each flight, customer satisfaction ratings per flight, number of customer complaints per flight, factors that contributed to customer complaints, weather reports, seasonal indicators, and time to resolution for customer complaints. With all of these various data points she might be able to predict the quality of a single flight and it's customer complaints. But there are hundreds of flights each day. The real value for Maya would come from being able to make predictive insights for all flights all year round. More importantly, it would be far more valuable if she could dynamically adjust pricing, or staff assignments, or even catering based on the predictions. ML unlocks these capabilities and more.

But what exactly is machine learning? To understand that, we need to step back and define artificial intelligence first.

>> Artificial intelligence or AI is a broad field or term that describes any kind of machine capable of a task that normally requires human intelligence, such as visual perception, speech recognition, decision-making, or translation between languages. Machine learning or ML is a certain branch within the field of AI. Specifically ML refers to computers that can learn from data and make predictions or decisions without being explicitly programmed to do so. This is done using algorithms or models to analyze data. These algorithms use historical data as input to predict new output values.

>> Machine learning solves many kinds of problems, for the purposes of this course will focus on the definition of ML that applies to numerical or classification problem types. I'll use this ML definition to guide your learning throughout the module, and here it is. ML is a way to use standard algorithms or standard models to analyze data in order to derive predictive insights and make repeated decisions at scale.

Put simply, it's a way of teaching a computer how to solve problems by feeding it examples of the correct answers. Usually these problems are about predicting something.

For example, you can predict how long it takes to travel from one location to another by feeding the computer examples of the completed journeys. Similarly, you can predict the estimated taxes owed by feeding the computer examples of tax filings. You do the same for predicting weather patterns over the next few days. More technically speaking, suppose you wanted to use machine learning to accurately label a photo of a fruit or vegetable the model has never seen before. You train an ML model or the standard algorithm using lots of images of fruits and vegetables, input data, and their correct labels, output data. As you train the ML model with more input data and corresponding output data, it's predictions become more accurate when you feed it an image of a fruit or vegetable it hasn't seen before. Now that was an overly simplified example, we'll cover many more real world examples throughout the module.

Ultimately, the purpose of ML in a business is the same as all other disruptive new technologies to enable organizations to better achieve their missions. To apply machine learning effectively, you need lots of data. In fact, you need lots of high quality data to generate more and more accurate meaningful predictions. In the next video, I'll examine factors that impact data quality.




Data Quality
In the previous video, you learned that ML is a way to use standard algorithms or models to analyze data. This analyzed data can be then used to derive predictive insights and make repeated decisions. The accuracy of those predictions, however, depend on large volumes of data that are free of bugs. Let me use a software analogy to explain what I mean by bugs. In traditional software development, a bug is a mistake in the code that causes unexpected or undesired behavior. In ML, even though there can be bugs in the implementation of an algorithm, bugs in data are far more common. Consider this example. A few years ago, some Googlers wanted to use ML to help diagnose diabetic retinopathy, which is the fastest growing cause of blindness, potentially affecting more than 415 million diabetic patients worldwide. Working closely with doctors in the US and India, these Googlers created an ML model that would diagnose diabetic retinopathy almost as well as ophthalmologists can. They trained an ML model using labeled images of the backs of eyes, each label being the diagnosis. Because humans were involved in the labeling of the images, the labeling system is not completely objective. The data may have included incorrect labels or even human bias, which is then propagated into the ML model itself. So how would you ensure that you have optimal data quality when training an ML model? The best data has three qualities. One, it has coverage. Two, it's clean or consistent. And three, it's complete. I'll explain each one. Data coverage refers to the scope of a problem domain and all possible scenarios it can account for. In other words, all possible input and output data. Let's imagine an auto manufacturing use case where the goal is to use ML to automatically identify defects in car parts. Let's assume also that the car parts are divided into red and blue. If red and blue make up all the possible scenarios, but you only train your model with red parts, the model might not be able to detect defects in blue car parts when it's presented with new data. So more data and broader coverage produce a more accurate ML model.

The second quality of good data is its cleanliness. This is sometimes called data consistency. Data is considered dirty or inconsistent if it includes or excludes anything that might prevent an ML model from making accurate predictions. This is a lot like the errors or bugs we talked about earlier. The simplest form of inconsistency in data is data format. Suppose, for instance, you want to analyze data from multiple documents, and one of the data points on each document is a timestamp. The timestamps from all sources have to be of the same format, otherwise the data is considered dirty. Let's return to our manufacturing scenario. Where do you think inconsistencies could occur? Well, if you're using photos to look for defects in car parts, you need to be careful with which images you choose to train the model. For example, if the images have shadows in them, the model won't know whether shadows are part of an object or not. If you want to make predictions from images that are supposed to have shadows, that's okay-- otherwise your data is dirty. I mentioned incorrect labels earlier, which is another form of dirty data. In this scenario, you might have parts that were labeled as fractured, but in reality they were discarded because they were the wrong size. There are lots of examples of human error that causes dirty data as well. Imagine the sales and retail industry, for example. If someone enters incorrect purchase data in a data storage system, this creates dirty data. If there's an error in an automated service, or if a transaction is recorded incorrectly every time the register runs out of paper, this also produces dirty data. The more incorrect or dirty data you have, the more correct and clean data you'll need to provide a counterbalance so the ML model learns the correct outcome. Another quality of good data is completeness. This refers to the availability of sufficient data about the world to replace human knowledge. Think of this as the various data categories or themes that help complete a user's profile such as address, gender, or height. Incomplete data can limit the performance of an ML model. We say there's incomplete data when there's a lack of better data, there are mistaken expectations about how ML works and what it's capable of, or program design and implementation are poorly executed. Let's go back to our manufacturing example. Imagine that one of the major sources of defects is overheating, but you're not collecting temperature data. That's an example of incomplete data. Even if you start collecting temperature data now, you may not have the historical data that maps to past examples of good and fractured parts.

Another form of incomplete data is the number of cases for all possible scenarios the data is intended to cover.

In the same manufacturing example, your goal is to match the labels, good condition and fractured, with every part. If axle is one item you're evaluating for defects, you'll need examples of axles in good condition and fractured. If you don't have that data, your data is incomplete. Remember--data is the tunnel through which your model views the world. Anything the model can't see it assumes doesn't exist. For example, if a model was given an image that only showed what's on the left, it might think the road was open and traffic free. In reality, if I show you the full image, the road is just closed. The good news is that most of these problems can be solved simply by getting more data, but you have to be purposeful in collecting that data.

Do you need to improve coverage, improve cleanliness or consistency, or improve completeness?

Remember--data is central to ML. If you're planning to use it, you'll need to account for as many possibilities when preparing your data before training an ML model.

Now you might be wondering what kinds of skills or expertise you need to begin using machine learning in your organization. Before you go too far down that path, I want to reassure you that ML has become more accessible than you think. In the next video, I'll explore some Google Cloud ML solutions that you can use-- some even right away-- to bring new value into your business.




3.3 Using AI and ML 

AI and ML with Google Cloud
Many people assume that you need a robust technical team of data analysts, data engineers, and even ML engineers to leverage the capabilities of Cloud in ML. But ML is even more accessible now than ever before. In fact, Google Cloud democratizes AI by providing a range of tools to support an entire machine learning workflow across different model types in varying levels of ML expertise. When you start a machine learning project, you can use this simple flow chart to help you identify the best approach for you. You need to start by asking yourself a few questions. First, do you have your own training data. If not, you need to use some of Google's pre-trained APIs to solve your problem. If you do have your own training data, then you'll use services within Vertex AI, a unified managed platform for building ML using Google Cloud. To identify what kinds of models you'll be building, you need to ask another question. Are you or your team writing the model code yourself? If not, then you'll train an existing ML model with your own data. If you are, you'll build a custom ML model and train it using your own data. Let's take a closer look at these three ML approaches starting with pre-trained APIs. If you don't have your own training data, you'll use Google Cloud pre-trained APIs. 

A great option if you don't have specialized data scientists, but do have business analysts and developers. This is the fastest and lowest effort approach, but is less customized. Google Cloud can help developers build smart apps using pre-trained APIs, which provide access to ML models for common tasks like analyzing images, video and text. APIs can be deployed in a virtual private Cloud, on-premises, or in Google's public Cloud. They can be used regardless of the level of ML expertise. These pre-trained APIs can be divided into four categories; sight, language, conversation, and structured data. To better understand this, let's imagine a developer building a mobile app that users will submit photos to. The app needs to recognize what the images are and filter out any not safe for work. The developer might choose Vision API. This API offers powerful, pre-trained and Machine Learning models, which use Google data to automatically detect faces, objects, text, and even sentiment in images. The developer can use Vision API to assign labels to images and quickly classify them into millions of predefined categories. 

The Natural Language API is another pre-trained API. Suppose your business has a contact form on its website which receives many messages every day. This data can be difficult and time insensitive to manually handle, categorize and action. Natural Language API discovers syntax entities in sentiment and text using Google Data and classifies text into predefined categories. In this case, it can decide if comments represent complaints, praise, an attempt to learn more about your business and more. Another more custom approach to train models with your own data using Vertex AI to manage and build your ML projects. Vertex AI brings together Google Cloud Services for building ML under one unified user interface. When working in Vertex AI, you can use your data to either train an existing ML model or to build a custom ML model. When you apply your own data to pre-existing Google ML models, you'll use AutoML on Vertex AI to train, test and deploy a machine learning model without code. This is a great option for businesses that are willing to spend a little more time and effort on producing a more customized ML model. Let's go back to our image recognition example which used Vision API. Imagine you work for a car manufacturing company. Vision API can help you tell the difference between generic images found in Google databases like the difference between a wheel and an engine. But it won't be able to identify good or defective specific parts. 

A developer could use AutoML Vision API and train it with custom data. This API automates the training of your own machine learning models. Meaning a developer can simply upload a custom batch of images or ingest them into AutoML Vision directly from Cloud storage and train an image classification model with the easy to use graphical interface. Models can be further optimized and deployed for use directly from the Cloud. Earlier I talked about the Natural Language API for processing entries in an online contact form. If your text examples don't fit neatly into the Natural Language API sentiment based or vertical topic based classification scheme, and you want to input your own data, you'll need to use AutoML Natural Language. AutoML Natural Language lets you build and deploy custom machine-learning models that analyze and categorize documents and identify entities or assess attitudes within them. You can use the AutoML user interface to upload your training data and test your custom model without a single line of code. These are just a few of the many Google Cloud ML offerings. You can also find APIs that categorize videos, convert audio to text or text to audio, translate between languages, and so much more. In many of the most innovative applications for machine learning, several of these applications are combined. 

Vertex AI is a platform for creating custom end-to-end AI models, providing a suite of products to help you at each stage of your ML workflow from gathering data to feature engineering, to building, deploying and monitoring models. We also have Google Cloud AI Hub, which hosts a repository, a plug-and-play AI components for developers and data scientists to use in their projects. With ML models that are fully custom built end-to-end, the process takes the longest and requires a specialist team of data scientists and engineers. However, these models are the most specialized to your needs and give your business the most differentiation and innovative results. Finally, beyond the build your own options, Google Cloud also offers a set of full AI solutions aimed to solve specific business needs. This includes Contact Center AI which transforms contact centers by providing models for speaking with customers and assisting human agents, increasing operational efficiency and personalizing customer care. Other solutions include Document AI, which uses AI to unlock insights from your documents. And Cloud Talent Solution, which uses AI with job search and talent acquisition capabilities. These are just three of many fully built AI solutions. 

Taking a step back from these ML approaches, you should know that all ML models are built on top of Google Cloud's AI foundational infrastructure. TensorFlow, an end-to-end open source platform for machine learning, is part of this foundation. It has a flexible ecosystem of tools, libraries, and community resources that allow researchers to innovate in ML and developers to build and deploy ML powered applications. TensorFlow was first developed for Google's internal use, but is now open source so everyone can benefit. It takes advantage of Tensor processing units or GPUs, which are hardware devices designed to accelerate ML workloads with TensorFlow by 15-30x. Google Cloud makes them available in the Cloud with Compute Engine virtual machines. This visual is a helpful way to understand how infrastructure, data analytics and ML fit together. Strong data is the foundation of good AI. But this is primarily built on a foundation of IT infrastructure, applications and data management. On top of this sits data lakes and data warehouses on which you can run data analytics to make data-driven decisions. On top of this is ML and data science made up of rapid prototyping and reproducible experiments. So as you can see, Google Cloud democratizes AI by providing tools to support the entire machine learning workflow, accommodating different needs and levels of ML expertise.




Real-world use cases for ML
So far, we've defined machine learning, reviewed the importance of quality data, and explored a few Google ML and AI offerings. In this video, I'll cover four common business problems that ML is particularly suited to solve. In each case, ML is uniquely placed to create new business value when it can learn from data to automate action and processes and to customize responses to behavior. The four common business problems are replacing or simplifying rule-based systems, automating processes, understanding unstructured data, and creating personalized customer experiences. Let's start with the rule-based systems. I'll use Google Search as an example. Suppose, for instance, you want to search for the Giants, a sports team. Ah, but wait, if you type in "giants," should the search results show you San Francisco Giants or New York Giants? One is a baseball team based in California, the other is an American football team based in New York. 

A few years ago, the search engine code base used hand-coded rules to decide which sports team to show a user. If the query is giants and the user is in the Bay Area, show them results about San Francisco Giants. If the user is in the New York area, show them results about New York Giants. If they're anywhere else, show them results about tall people. This is for just one query. If you multiply this by thousands of different queries and by different users each day, you can probably imagine how complex the whole code base would become. This is a perfect problem for ML to solve. If we had all of the data that tells us which search results users clicked on per query, why not train a machine learning model to predict the rank for search results? That was the idea behind RankBrain, Google's deep neural network for search ranking, which was introduced in 2015 by Google's engineers. It outperformed many human built signals and, using ML, Google was able to replace many of the hand-coded rules. 

The neural network ended up improving search quality dramatically. In fact, Google's neural network is a key differentiator among similar technologies in the market. An added benefit of RankBrain or any machine learning model is that the system could continually improve itself based on new user queries and new user clicks. Search is one example of how ML leverages vast amount of data to provide highly accurate predictions in a rule-based system. A second opportunity for using machine learning is to automate processes where ML makes predictions and repeated decisions at scale. Let's look at an example. Ananda Development is a property developer headquartered in Thailand that decided to use ML to automate the handover stage of their property sales. Before embracing cloud and ML, the handover process included multiple manual steps and was prone to errors. In any sale before the buyer paid for the property, an Ananda development inspector and the buyer had to conduct a detailed check of the condominium for any building variations that needed to be fixed.

Ananda development inspectors would visually check hundreds of items a day for problems and list any issues on paper.

Prospective buyers might also take notes and photographs of the findings. On average, a single inspector would have to check several hundred items per day. Multiplied across several inspectors and multiple projects, this workload adds up.

This laborious manual process was also subject to occasional human error. That meant data could be omitted or recorded incorrectly. Ananda Development decided to build an app using machine learning to make the inspection process more efficient. The app used Google Speech-to-Text API to recognize and convert Thai language speech in a version of English spoken by many Thai people to text. The company found the product had an accuracy rate of over 90 percent in recognizing Thai speech and high accuracy rates in recognizing Thai English.

The inspection process is now more efficient and accurate. As another benefit, buyers also receive copies of electronic inspection reports and updated status notes as defects are repaired.

Another class of ML use case is for understanding unstructured data like images, videos, and audio.

Before I dive into examples of how you can use ML to understand unstructured data, I need to acknowledge a key point. So far in the module, we've been talking about a specific type of ML that uses structure data to make predictions at scale. Now I'm going to cover how ML can also be used to understand unstructured data.

Unstructured data is data that can't be directly compared to other data. For example, some characteristics of books are structured like the title, their publisher, location of publishing, number of pages. Again, this is known as tabular data, but it's not easy directly to compare the content of the two books or to precisely determine how they are related or different. Even human experts might not agree on exactly how similar two books are. Open text or language is just one example of unstructured data. Other examples include pictures, videos, and audio. A great example of using ML for unstructured data comes from Ocado. Ocado is one of the world's largest online-only grocery supermarkets. Previously, all email sent to Ocado would go to a central mailbox for sorting and forwarding by a person. This process was time-consuming and would lead to a poor customer experience.

To improve and scale this process, Ocado used ML to automatically route emails to the department that needs to process them. This new process eliminated multiple rounds of reading and triaging. Here, Ocado used ML to both automate a process and understand unstructured data. Specifically, they used ML's ability to process natural language to identify the customer sentiment and the topic of each message so they could route it immediately to the relevant department.

Now let's look at a fourth example, personalization. Many businesses use ML to personalize user experiences. Personalization is the difference between a newspaper and an email. A newspaper article can be interesting, but it's written to appeal to thousands or millions of people. However, an email is often tailored just to one person by including their name, for example. YouTube is a great example of personalization in action. When you watch a video on YouTube, you're probably noticed on the homepage or to the right of your video, there's a list of recommended videos that are up next. When your video finishes, these new videos will play. And we'd like them to be interesting and useful for you. This feature keeps the user interested and engaged with the product. 

By providing personalized recommendations using ML, YouTube can deliver a better service to their customers while also increasing their ad revenue. Many businesses use the same approach to feature product recommendations on their websites personalized to individual users. Other businesses use personalization to surface new content like music recommendations or films to stream. Great, let's do a recap. So far, I discussed some common applications of machine learning such as replacing rule-based systems, automating business processes, understanding unstructured data, and personalizing applications. It's important to remember that ML models aren't standalone and that solving complex business challenges requires combinations of models. There are, of course, many more applications of machine learning for businesses, and you can learn even more about them in the course Managing Machine Learning Projects with Google Cloud. Next, I'll summarize the key topics we covered in this course.





































#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#




Infrastructure and Application Modernization with Google Cloud

Course Introduction
Hello, and welcome to the Value of Infrastructure and Application Modernization with Google Cloud. I'm Saman Javan, lead course developer and certified facilitator here at Google Cloud. Consumer expectations over the last 20 years have radically changed. Customers now expect connected digital experiences in real time. Many businesses, especially large traditional enterprises, built their IT infrastructure on premises. Legacy systems and applications make up the organization's IT backbone. At the same time, these legacy systems and applications struggle to achieve the scale and speed needed to meet modern customer expectations. Business leaders and IT decision makers constantly have to choose between maintenance of legacy systems, and investing in innovative new products and services. In this course, I'll explore the challenges of an outdated IT infrastructure, and then describe how businesses can modernize that infrastructure using cloud technology. In Module One, I'll introduce infrastructure modernization as the core topic, In particular, I'll examine compute options available in the cloud and the benefits of each. I'll also present a few Google cloud solutions and highlight customers who have successfully used them.

In Module Two, I'll focus on application modernization. Applications are not new in the cloud. But cloud technology enables businesses to develop, deploy, and update applications with speed, security and agility built it. I'll also cover App Engine, a Google Cloud solution that lets application developers build scalable web and mobile applications on a fully managed serverless platform. In the third module, I'll present application programming interfaces, or APIs, and explain how they unlock value from legacy systems, enable businesses to create new value, and monetize new services.

I'll also cover Apigee, a Google Cloud Platform for developing and managing APIs. I'll then close the course summarizing the key points and offer additional resources for you to continue your learning. And remember, you don't need to be an IT specialist to create new business value or to develop innovative services. By understanding how infrastructure, applications, and APIs work together, you can initiate conversations about new projects and be more knowledgeable about strategic planning for digital transformation.




1.1 Overview

Introduction
Hello and welcome to the first module, Modernizing IT Infrastructure with Google Cloud. New businesses born in the cloud are challenging old business models. Scale is no longer a competitive advantage, it's the norm. Many organizations are very aware of this threat coming from digital disruption. What organizations want to know is how to best respond to this threat. How can they survive and thrive in this new cloud era? Central to an organization's ability to thrive in the new era is the way in which they structure and use their IT resources. This could mean moving away from investing resources to run and maintain existing IT infrastructure, to focusing more on creating new higher value products and services. With Cloud, organizations can develop and build new applications to drive better engagement with customers and employees. Foster securely and at scale. And ultimately, leveraging cloud technology to truly transform a business requires new collaborative models, changing culture and processes, and enabling team productivity and innovation. Enterprises are also seeing significant financial benefits from adopting Cloud as their approach to IT moves from buying fixed capacity to paying only for what they use changing the economics of technology investment. For many businesses, infrastructure modernization is the foundation for digital transformation. And with that, here's what I'll cover in this module. I'll begin by explaining what it means to modernize an IT infrastructure and why it matters. Then the different compute options available. Next, I'll cover private, hybrid and multi-cloud architectures and what we mean by each of them. I'll briefly go over Google Cloud's global infrastructure and close with Google Cloud computing solutions for setting up or modernizing the IT infrastructure.




Infrastructure Modernization
For most organizations, owning and operating infrastructure does not differentiate their business. In fact, it's often a burden. It limits an organization staff in several ways. For example, they have to undertake laborious task related to infrastructure, procurement, provisioning and maintenance. They are using legacy systems that are old. Don't add value to the business other than keeping the lights on and don't support business change. They cannot scale with any ease because they're locked into what they have on premises and forced to pay to over provision for peak usage.

One option for reducing this burden is to outsource the company's IT Infrastructure as much as possible and migrate to the cloud. But before we talk about migrating to the cloud, let's go back to a time before the cloud. All demonstrate how technology has impacted company business models over the years and use a simplified IT backbone to talk through the various changes. First let's look at employees, the technology users. These people use or create applications on laptops or computers and as part of their day to day work, they're storing data or files and connected to each other over the Internet. As the company grows and there is a need for more computers with more processing power, a company might have a data center with servers. Organizations might own their servers, data centers, cooling systems, the physical security features in place and the real estate to house all of that infrastructure. On top of this, they have to pay for maintenance and ongoing security costs.

Think of this as similar to owning a house, you're responsible for all of the infrastructure, the bricks and mortar, the fence around your garden, the locks on your door and all of the ongoing costs such as your utilities as well. The first step in moving away from what we call it on premises, infrastructure is co location. Here, a business sets up a large data center than other organizations around part of that data centre. This means organizations no longer have to pay for the costs associated with hosting the infrastructure, but they still need to pay to maintain it. It's like owning an apartment in a serviced apartment complex or house in a gated community. You've paid for some infrastructure, the apartment or the house and you're still responsible for maintenance. For example, if your heater breaks down. But some things like the perimeter security are outsourced. With both on premises and co location, value creation only starts well after a substantial amount of capital expenditure or Capex is committed. Given that hardware is often heavily underutilized even in the co location model, engineers found a way to package applications and their operating systems into what we call a virtual machine. Virtual machines share the same pool of computer processing storage and networking resources. Virtual machines optimize the use of available resources and enable businesses to have multiple applications running at the same time on a server. In a way that is efficient and manageable.

Most companies use virtual machines to optimize their use of data centers, whether on premises or co located.

The problem, though, is that there is still a cap to the physical capacity of existing servers. And companies still have to commit to a substantial amount of capital expenditure upfront. Many companies are now outsourcing their infrastructure entirely. They are growing to deliver their products and services to customers regionally and globally. Any to scale quickly and securely setting up and maintaining data centers and network connections that are optimal for their needs is expensive.

They don't see the benefit of owning their own data centers if they can outsource to a public cloud that offers infrastructure as a service.

In our analogy, this is like renting an apartment in a service building. Now if your heater breaks it's your landlord who is responsible for getting it fixed.

This means IT cost shift from being capital expenditure heavy to being more operational expenditure heavy. Outsourcing your I T needs at the infrastructure level is called infrastructure as a service. And public cloud providers such as Google Cloud offers several services to help you modernize your infrastructure. If your organization chooses to, it can move some or all of its infrastructure away from physical data centers to virtualized data centers in the Cloud. Google Cloud provides you with compute storage and network resources organized in ways familiar to you from your experience with physical and virtualized data centers. The maintenance work is outsourced to the public law providers. So it's easier to shift larger portions of company expertise to build processes and applications that move the business forward. Outsourcing IT resources gives the company flexibility but requires its teams to continue managing things like web application security. That is the information security that specifically deals with websites, web applications and web services. In this scenario, you would pay for resources, you allocate, for example a set number of virtual machines. If you want a more managed service, Cloud service providers offer something called a platform as a service.

In this case, you don't have to manage the infrastructure and for some services you only pay for what you use. As cloud computing has evolved, the momentum has shifted even further towards managed automated infrastructure and services. Google Cloud, for instance is known for its global access to a pool of configurable resources for every layer of the IT Infrastructure in the form of paid services. All right, now that you understand infrastructure as a service and platform as a service, let's look more closely at compute options. I already mentioned virtual machines as one method for optimizing the use of IT resources. In the next video, all examined VMS further and explore alternatives.




1.2 Understanding compute and Cloud options

Understanding compute options in the cloud
In the last video I covered some of the key advantages of using public cloud services to modernize or even set up your IT infrastructure. First, cloud reduces the need for IT teams to act as a gateway to technical resources such as network, security storage, compute power and data. Think of the cloud as an on demand self service for anyone in the business. Next there is a broad network access. This means that access to data and compute resources is no longer tied to a particular geography or location. Now teams can access compute resources and data with little to no latency.

Third resources are distributed across a global network of data centers. If one is down due to a natural disaster for instance another data center is available to prevent service disruption. This is referred to as resource pooling.

Next companies can scale up or down instantly due to the availability of on demand. Cloud resources there is a rapid elasticity means businesses can serve their customers without interruption in a cost effective way.

And finally cloud is a measured service which means companies have a lower up front or capital expenditure because they don't have the need to purchase their own data center equipment or maintain their IT infrastructure. If you've decided to modernize your business IT infrastructure, you might be wondering what options are available to you in this video all explored the three main options that you can use to modernize your infrastructure. Virtual machines, container realization and server list computing. I'll also touch on kubernetes, a solution for managing your services and machines. First, let's make sure we have a shared understanding of key terms in the context of the cloud compute or computing refers to a machine's ability to process information to store, retrieve, compare and analyze it and automate tasks often done by computer programs otherwise known as software or applications.

Traditionally, the hardware available for computing could only run a limited amount of software and applications as you learned in the last video virtualization changed. This virtualization is a form of resource optimization that allows multiple systems to run on the same hardware. These systems are called virtual machines, or VMS.

This means they share the same pool of computer processing storage and networking resources. VMS enable businesses to have multiple applications running at the same time on a server in a way that is efficient and manageable.

The software layer that enables this is called a hyper visor. Hyper visor sits on top of physical hardware, and multiple VMS are built on top of it. It's like having multiple computers that only use one piece of hardware. Virtual machines are the first compute option for infrastructure modernization. The second is containers, containers follow the same principle as virtual machines. They provide isolated environments to run your soft for services and optimize resources from one piece of hardware. However, there are even more efficient virtual machines recreate a full representation of the hardware. By contrast, containers only recreate or virtualized the operating systems. This means that they only contain exactly what's needed for the particular application that they support. Containers offer a far more lightweight unit for developers and IT operations teams to work with and provide a range of benefits. 

They start faster and use a fraction of the memory compared to booting an entire operating system, containers give developers the ability to create predictable environments that are isolated from other obligations. Let me use an analogy to explain the advantage of containers. Suppose you want to build an apartment block. one way to do this is to start with the steel beams, then build the outside walls, then run the electricity and plumbing, then build the interior walls. However, if you discover a fault somewhere in the building, it can be very difficult to isolate the problem because everything is connected, adjusting features of each apartment or fixing problems can be challenging and expensive. Another way of building an apartment block is to use prefabricated units. In other words, you build the units off site and then essentially lay them on top of each other.

This means that any problem that arises is easier to isolate and fix. It also means that individual apartments can have unique designs with different features because they are all compartmentalized rather than one giant unit.

This is what containers do for your applications. So if a customer asks for a new feature or a change in the application, your developers can easily make an update to that particular part of the application without affecting the rest containers are able to run virtually anywhere, which makes development and deployment easy. They can run on Linux, Windows and Mac operating systems, on virtual machines, bare metal, which means directly on the hardware, on a developers machine or in data centers on premises. And of course in the public cloud containers, improve agility, strength and security, optimize resources and simplify managing applications in the cloud. Many businesses have a mix of VMS and containers. However, as their IT infrastructure set up becomes more complex, businesses need a way to manage their services and machines. For example, businesses can have millions and millions of containers. This means that keeping them secure and making sure that they operate efficiently can require significant oversight and management. Kubernetes is an open source cluster management system that provides automated container orchestration.

In other words, kubernetes simplifies the management of your machines and services for you.

This improves application reliability and reduces the time and resources you need to spend on development and operations, not to mention the relief from the stress attached to these tasks. Kubernetes makes everything associated with deploying and managing your application easier. We'll explore kubernetes and google kubernetes engine more in module two. When we examine application development finally, the third compute option is serverless Computing. Serverless computing doesn't mean there's no server though. Serverless computing means that resources such as compute power are automatically provisions behind the scenes as needed.

This means that businesses do not pay for compute power unless they're actually running a query or application. At its simplest server list means that businesses provide the code for whatever function they want and the public cloud provider does everything else. Let me give you an example, imagine you're a healthcare technology company, you help general practice doctors to seamlessly connect with their patients. One tool you provide is an application for patients to book appointments with their doctor. You want to add a feature that enables patients to upload an image with their appointment booking. In this case the ability to upload an image is called a function. You, as the healthcare technology company, write the code for that function directly into your public cloud platform. 

The public cloud provider manages everything else. For this reason, serverless computing solutions are often called function as a service. Some functions are response to specific events like file uploads to your cloud storage or changes to your database records. You write the code that defines the response to those events and your cloud provider does everything else. Ultimately, every business has different compute requirements based on where they are in their cloud adoption journey. As such, determining the right blend of compute solutions is a necessary part of any business cloud strategy.

Now, before we talk about google cloud compute solutions, I want to cover a key dimension of your cloud strategy. That is your service architecture.




Hybrid and multi cloud
Today, most of the world's enterprise computing still happens on premises. It hasn't moved to the cloud yet because the path forward is complex daunting and full of difficult decisions. Sometimes workloads remain on premises due to compliance or operational concerns. So how do you modernize the infrastructure you have without jumping completely to the cloud? How do you bridge incompatible architectures while you transition? How do you maintain flexibility and avoid locking? Although there are many benefits to developing cloud first or cloud native applications and systems. Many enterprises have complex needs that will involve some on premises infrastructure working in conjunction with public cloud services provided by companies like google cloud. 

Before we go any further though, let's make sure we're using a standard definition for the following terms. Private cloud, Hybrid cloud and Multi cloud. Private cloud is where an organization has virtualized servers in its own data centers to create its own private on premises environment. This might be done when an organization has already made significant investments in its own infrastructure or if for regulatory reasons, data needs to be kept on premises. Hybrid cloud is when an organization is using some combination of on premises or private cloud infrastructure and public cloud services.

This is the situation many organizations are currently in some of their data and applications have been migrated to the cloud. Others remain on premises and interconnects between the private and public clouds allow interoperability. Multi cloud is where an organization is using multiple public cloud providers as part of its architecture, in this case, the organization needs flexibility and secure connectivity between the different networks involved. An organization might choose to use either hybrid cloud or multi cloud if they want to incorporate specific elements of a public cloud in order to take advantage of the key strengths of that provider. For example, many organizations see enormous benefits from Google's big query data analytics tool. A server list application that scales to multi petabyte data sets but may keep the core applications generating data that needs to be processed on premises. 

When organizations are considering a move to a hybrid cloud or multi cloud situation, they are often concerned about how easy will be to move an application from one cloud to another. Google believes that being tied to a particular cloud shouldn't get in the way of you achieving your goals. Instead, Google believes in an open cloud where users have the rights to move their data as they choose if organizations have the power to deliver their apps to different clouds while using a common development and operations approach. This will help them meet their business priorities and rapidly accelerate innovation. Open source in the cloud preserves and organizations control over where they deploy their IT investments. Let's look at some examples because google Cloud uses open APIs. Google services are compatible with open source services and products. This means you can take the code from, let's say Google's cloud Big Table, a managed database and use that code somewhere else. Because Google Cloud publishes key elements of its technology using open source licenses, customers can use products both on premises and on multiple clouds. One example of an open source service you may have heard of is Tensor Flow, an open source software library for machine learning developed inside Google.

Another you may have heard of is Kubernetes, a system for automating, application, deployment, scaling and management using a concept known as continue realization. Finally, google cloud has created Anthos an open application modernization platform that enables you to modernize your existing applications, build new ones and run them anywhere. It allows you to build an application once and run it wherever you want on premises on google cloud on a different public cloud. This will help accelerate application development for your organization. These examples of open source solutions in the cloud enable businesses to leverage google cloud infrastructure and deploy applications using google cloud solutions on premises and are using another cloud provider. The reliability and resilience of the cloud infrastructure is critical to business operations and success. Now, another key component of a cloud strategy is a secure network. Google's network carries as much as 40% of the world's internet traffic every day.

In fact, Google's network is the largest of its kind on earth and google has invested billions of dollars over the years to build it. Google cloud customers are able to run their applications and services on the same infrastructure that google uses to serve billions of users around the world.

The network is truly global, operating in over 200 countries and territories with 20 regions and over 130 points of access. This means that customers benefit from a private, well provisioned, highly reliable global network. Now you might be considering multiple factors as part of your cloud strategies, such as cost security, openness and of course, the value of available products and services. Perhaps like assad google, you're taking the environment into consideration too. By moving compute from a self managed data center or co location facility to google cloud, the net emissions directly associated with your company's compute and data storage will be zero. Why, because Google Cloud matches 100% of the energy consumed by our global operations with renewable energy and maintains a commitment to carbon neutrality. So when you use Google cloud to store your data and develop your applications, for example, your digital footprint is offset with clean energy, which reduces your impact on the environment. The takeaway is that every organization needs to think about their cloud strategy and understand the available options. Google Cloud provides a range of infrastructure solutions to help businesses modernize and better serve their customers. In the next video, I'll cover what those solutions are by category.




1.3 Google Cloud Compute Solutions
So far, you've learned about the benefits of infrastructure modernization, the various compute options available including virtual machines, containerization, and serverless computing, you've learned the difference between private hybrid and multi-Cloud strategy, and the benefits of the global infrastructure that google Cloud provides. Now, let's look at some specific Google Cloud solutions. In this video, I'll cover VM based compute options including Compute Engine, Google Cloud VMware Engine, and Bare Metal. Next, I'll look at Google Kubernetes Engine or GKE, which is a container-based compute option. Finally, I'll explore three serverless computing solutions, App Engine, Cloud Functions and Cloud Run. Let's start with Compute Engine, which is a computing and hosting service that lets you create and run virtual machines on Google's infrastructure. Compute Engine delivers scalable, high performance virtual machines running in Google's innovative data centers and worldwide fiber network. 

Compute Engine VMs boot quickly, come with persistent disk storage, and deliver consistent performance. This solution is ideal if you need complete control over the virtual machine infrastructure. It's also useful if you need to run a software package that can't easily be containerized or have existing VM images to move to the Cloud. To better understand how Compute Engine works, let's turn to an example of a company that use this option to overcome challenges and scale their business. Spotify had reached a tipping point with their current business model where it wouldn't be able to scale any further. By leveraging Compute Engine, Spotify was able to effortlessly scale their business to reach millions of users. Google Cloud has allowed Spotify to build the audio network of the future and continue innovating all while providing users with billions of unique experiences. Another VM based solution is google Cloud VMware Engine. 

This is a type of software that you can run on a virtual machine. Google Cloud VMware Engine is a fully managed service that lets you run the VMware platform in Google Cloud. Google manages the infrastructure, networking, and management services so that you can use the VMware platform efficiently and securely. An example of a company that uses google Cloud VMware Engine is DBG, one of the world's leading exchange organizations. They use Google Cloud as the foundation for scalable, resilient, and compliant infrastructure for financial markets. Using Google Cloud VMware Engine, DBG was able to spin up a new private Cloud in under 40 minutes with minimal disruption. This enabled them to scale their business on-demand and meet customer needs while still using their VM tools and existing processes. The final VM based compute solution we'll cover today is Bare Metal. You can migrate many existing workloads to the Cloud easily. 

However, some specialized workloads are difficult to migrate to a Cloud environment. These workloads required hardware and complicated licensing and support agreements. Bare Metal enables you to migrate specialized workloads to the Cloud while maintaining your existing investments and architecture. This allows you access to an integration with Google Cloud services with minimal latency. Next, let's look at the google Cloud container based solution, Google Kubernetes Engine, often shortened to GKE. Google Kubernetes Engine or GKE, provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The GKE environment consists of multiple machines, specifically Compute Engine instances grouped together to form a cluster. GKE allows you to securely speed up app development, streamline operations, and manage infrastructure. An example of a company that used GKE to improve their business is Current. 

Current is a financial technology company that offers a debit card an app made for teenagers. Current uses GKE to improve time to market for application development by 400 percent while eliminating downtime for users. Finally, I'll cover three serverless computing solutions. Let's start with App Engine. Google App Engine is a platform as a service and Cloud computing platform for developing and hosting web applications. App Engine, let's app developers build scalable web and mobile backends in any programming language on a fully managed serverless platform. This means app developers can focus on writing code without having to manage the underlying infrastructure. IDEXX Laboratories Inc. develops and manufacturers veterinary care products and technologies, including diagnostic tools and information technology. IDEXX used Google App Engine to launch Vet Connect plus an app that gives veterinarians anytime, anywhere access to clinical decision support data that keeps pets healthy. 

By leveraging Google App Engine, IDEXX Laboratories was able to save up to $500,000 in annual IT spent. Now let's look at another serverless computing solution. Cloud Run. Cloud Run allows developers to build applications in their favorite programming language with their favorite dependencies and tools and deploy them in seconds. Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantly depending on user traffic. But how can Cloud Run improve businesses and offer real-world solutions? Well, Veolia group provides access to water, waste, and energy resources for millions of people across 52 countries. It develops sustainable solutions to preserve and replenish these resources across communities and industries. 

By leveraging Cloud Run for their algorithms, Veolia has benefited from automatic scaling, multiple route support, and fast deployments, all while saving money. The 3rd serverless compute solution is Cloud Functions. Cloud Functions is a serverless execution environment for building and connecting Cloud services. It offers scalable pay as you go functions as a service to run your code with zero server management. Cloud Functions offers a simple and intuitive developer experience. You or your developers can simply write your code and let Google Cloud handle the operational infrastructure. 

With Cloud Functions, developers are also more agile as they can write and run small code snippets that respond to events. Lucille Games is a good example of a company that optimized their business by harnessing Cloud Functions. Lucille Games used Cloud Functions and other google Cloud solutions to build apps, run servers, and create original games that can scale to millions of users on-demand. 

As I mentioned before, infrastructure modernization serves as the foundation for digital transformation. It's important to think carefully about your Cloud strategy and what compute options you can leverage, how you build your architecture, influences how your business harnesses applications, manages data, and ultimately develops and thrives with this ever evolving digital age. Whether you're able to embrace innovation or whether you're constrained by your Cloud environment is determined by the choices you make now. In the next module, we'll explore another important factor in your Cloud adoption journey, application development. Leveraging the right applications in your business can transform how you work and unlock new value. An application development in the Cloud doesn't belong exclusively to the IT team. 




2.1 Overview
Now on to Module 2, modernizing applications with Google Cloud. In the last module, you learned about infrastructure, which is the IT backbone of any business. In this module, we'll focus on applications. First, what are applications exactly? Simply put, applications are computer programs or software that help users do something, and in this digital age, they're everywhere. Think about how many applications you interact with on a day-to-day basis. You check your emails or scroll through your social media via an app. Perhaps you track your fitness with wearable technology that links to an app on your phone. You might even create and share content with your colleagues via specific applications. The list is endless. Customers now expect intuitive, well-functioning applications that enable them to do things faster. A businesses capacity to meet that demand influences their ability to thrive in the Cloud era. 

Applications, however, aren't possible because of Cloud technology. Applications have been developed on-premises for years and still are, but on-premises application development often slows businesses down. This is because deploying an application on-premises can be time consuming and require specialized IT teams. Any new changes can take six months or even more to implement, this then creates friction within different parts of the business. For example, customer facing teams that want specific features might be delayed by developers who struggle to make updates fast enough. Or developers that really want to be innovative and try new things might be inhibited by operations teams that are concerned about the stability of existing applications. 

Cloud technology enables businesses to develop and manage applications in new ways so they're more agile and responsive to user needs. In other words, businesses are able to develop applications and deliver updates quickly and respond to the needs of their customers rapidly. In this module, I'll explore how businesses can modernize their existing applications and build new ones in the Cloud. In particular, I'll cover five common change patterns for businesses who want to modernize their applications. Next, I'll consider some key application development challenges that businesses face. Then I'll highlight two Google Cloud solutions, Google Kubernetes Engine and App Engine. 




2.2 Modernizing Applications

Cloud change patterns
It's often assumed that modernizing an application with Cloud technology can only be done in one way. Move everything to the Cloud all at once. Yikes. That can be risky, especially for large applications. The good news is, that's just one approach. Moving an application to the Cloud doesn't need to be done all at once. Google Cloud has identified five common patterns that businesses can adopt when they want to modernize their applications. A business can move applications to the Cloud first and then change them, or they can change their applications before they move, or they can invent in greenfield or invent in brownfield or they can just move their applications without any changes. Let's look at each of these in turn. If an organization wants to take a relatively conservative approach to modernizing applications with the Cloud, they might take a move first, then change approach. This path typically starts with a lift and shift program for selected applications. The migration of these applications typically brings minimal changes to the ways of working within the organization. But once the applications are running in the Cloud, they are then ready to be updated more easily than when they were running on-premises. For example, a legacy application that is moved to the Cloud could have its security improved by using the enhanced firewall and identity access management, IAM, capabilities of Google Cloud. Over time, further modernization can be explored, potentially using APIs to change the way that the application interacts with data and other applications, or even making the application serverless so that it can become Cloud-native, event-driven application. The most efficient form of application architecture. After the first set of applications have been re-architect and optimized in the Cloud, further applications can be moved. Think of this like renovating your house to maximize your space. You don't have to renovate every room all at once, and you don't want to completely redo every room either. You can start to make changes as you're ready for them based on your needs and budget. Suppose for instance, you want to start with the kitchen. You could replace the kitchen cabinets and counter-tops, and still continue to use the electronic oven as is. Eventually, after you've put in a gas line, you can then replace your electric oven with a gas rage. If an organization wants to take a more aggressive approach to modernizing its applications, they can re-architect applications first to make them more Cloud-ready before migrating them. For our analogy, that may mean completely changing the design of the kitchen and the placement of the appliances for maximum efficiency, and buying brand new appliances for the new design before doing the renovation work. For some organizations, their initial interest in the Cloud is because they want the ability to build new innovative applications quickly. They may not want to or be ready to move existing applications at this point. So when we talk about a greenfield strategy, we're talking about building an entirely new infrastructure and applications in the Cloud. It's like creating an office and buying furniture for it as part of your renovation project when you don't currently have an office in your existing home. This approach really only applies when an organization needs to develop new products or offerings, such as a B2C bank that wants to develop its digital banking channel. The organization doesn't need to touch all your applications just yet. They could take either the move and change or change and move approach if they decide to modernize them at a later point. Inventing in greenfield allows you to build that innovative application that will help drive the business forward. But it does require agility, access to a diverse development skill-set, and strong support from leadership. A brownfield strategy, on the other hand, is to invent a new application in the Cloud environment that will replace an existing legacy application that remains on-premises. The legacy application is only retired after the new application is built. In our analogy, it's like creating a new office in your house while continue to use the cluttered desk space in the corner of a living room. You don't move any furniture or reorganize your documents until you know the new office space is set up. Although this redundancy can be comforting through minimizing risk, especially for mission-critical applications, there are increased costs associated with running applications in both places. Finally, it's worth noting that building Cloud-native applications isn't appropriate for all scenarios. For some use cases, it's sufficient to leverage the Cloud just to modernize the infrastructure layer as we discussed in the previous module. One possible use case is Cloud storage for data to allow organizations to decommission on-premises data centers. Another use case is modernizing the infrastructure only to allow organizations to create a virtualized environment for disaster recovery. Over the next few videos, we'll look at how the Cloud can support application development and maintenance.




Challenges in application development
Many business professional share similar concerns around application development processes and timelines. Creating a new application within an organization can be a challenge. Have you had the experience of going to your tech team and suggesting a new application. Only for them to tell you it will take 18 months, or maybe even tell you it's not possible with the legacy systems already in place. Traditionally, when business professionals want a new application, that tech or IT team has to do a lot of work to identify features, estimate capacity, define a technical architecture, consider integration with other systems and allocate resources even before a line of code is written. Once the requirements are agreed on, a new application will have to be designed, built, tested, integrated, and deployed. But new needs often compete with existing projects for time and resources. For some teams, this means spending just as much time creating and managing environments as is spent building business value. Whether building an app on-premises or in the Cloud, developers still need to make decisions about overall network architecture, choice of database, and type of server. All of these can slow down the application development process and even the launch of applications. The challenges for building apps using an on-premises infrastructure can outnumber those of Cloud native apps and can often be frustrating for developers and business professionals. Developers want to be creative and innovative by building new solutions, not spending hours maintaining the infrastructure. When developers get too far removed from the task they enjoy. Naturally, they start to seek out more interesting job opportunities that allow them to focus on building new apps and technologies. In addition to losing a key team member that needs to be carefully replaced quickly, the organization loses the intangible knowledge that good developers take with them when they leave. Developing Cloud native applications avoids the hassle of trying to create something that is constrained by legacy systems and outdated processes. Building a new application in the Cloud means you can be more agile in your development. It frees teams up from worrying about environment so that they can focus on creating features, which is where customers will get real value. Updating already existing applications that have been typically built on-premises presents difficulties too. Often, an application has been built with a monolithic architecture. This means that as it's updated over time, it's code-based becomes bloated, making it difficult to change something without breaking something else. What an application is updated, the entire application needs to be deployed and tested even if the change is only small. This makes implementing updates a lengthy and potentially risky process. When building new applications or modernizing existing ones a microservice architecture can reduce these problems. This type of architecture involves the separation of a large application into small, loosely-coupled services. The code-base for each service is modular, so it's easy to determine where the code needs to be changed. When a code change is required the service can be updated and deployed independently. In addition, each service can be scaled independently depending on its specific requirements. Adopting an automated continuous integration and continuous deployment approach also known as CI/CD can help you increase your application release velocity and reliability. With a robust CI/CD pipeline, you can test and roll out changes incrementally instead of making big releases with multiple changes. This approach enables you to lower the risk of regressions, debug issues quickly, and roll back to the last stable build if necessary. It also means you can update applications without interrupting services to your users. Imagine being able to deliver new features to your customers every day instead of a few times a year. Here's the important bit. Some organizations have been able to adopt CI/CD to build applications faster, but not always with the high-quality that customers demand. This is because they don't invest enough in building quality into the process. When building an application, you need to consider how quickly your systems can recover from downtime. If you're not able to recover from production infrastructure failures quickly, it doesn't matter how quickly you deliver software. You won't be able to deliver better customer experiences. Google Cloud developer tools help you release software at a high velocity while balancing security and quality. There are two tools we'll look at in this module: Google Kubernetes Engine and App Engine. You might remember Google Kubernetes Engine from the last module. Let's explore how it enables businesses to be more agile in app development.




2.3 Google Cloud Solutions

Google Kubernetes Engine
In the last module, I introduced containerization as a one compute option for modernizing IT infrastructures. In the context of application development, containerization allows developers to divide an application design into individual compartments. The advantage is that parts of the code can be updated without affecting the whole application. It also builds resilience because one error doesn't impact the whole application. As businesses create and scale more and more applications, they need a way to manage or orchestrate their containers. Kubernetes is an open-source container orchestration system for automating computer application deployment, scaling, and management. Google developed Kubernetes originally to support their own internal operations. As with so many innovations, Google then made it available to anyone as open-source technology. Google Kubernetes Engine, often shortened to GKE, is the Google Cloud managed service for container orchestration. GKE enables rapid application development and iteration by making it easy to deploy, update, and manage your applications and services. Now, you might remember serverless computing from our previous module as another option for modernizing IT infrastructures. The same option can be used for application development. With serverless computing, your cloud provider manages even more of your architecture. You write the code for the functions you want and the Cloud provider updates and adapts the container or VMs as needed to make that change. Let's move on to the second Google Cloud solution for app modernization. App Engine. It leverages serverless computing to enable businesses to develop applications. Click on the next video to find out more.




App Engine
I mentioned App Engine in the last module. App Engine is a platform for building scalable web applications and mobile back-ends. It allows you to concentrate on innovating your applications by managing the application infrastructure for you. For example, when you're building an application, App Engine manages the hardware and networking infrastructure required to run your code, so developers no longer need to spend valuable time doing this. During deployment, App Engine will scale your application automatically in response to the amount of traffic it receives, so you only pay for the resources you use. Just upload your code and Google will manage your apps availability. You can easily run multiple versions of your app to test new features or designs with end users, and because there are no service for you to provision or maintain, the monitoring and maintenance processes are easier too. Let's look at an example. Edp is one of the world's leading utility companies with a presence in countries across Europe, North and South America, and Asia. It's an end-to-end operator involved in the generation, distribution, and trading of electricity and gas. As a large company responsible for diverse operations, Edp has a complex IT infrastructure with over 400 applications. Many of Edp's IT systems were legacy systems not designed to integrate with one another, leading to inefficient delivery of data. In particular, Edp was experiencing problems with the performance of its customer account mobile app, which allows customers to check their usage, account, and payment details for their electricity and gas accounts. Edp also needed additional capacity to meet peaks and demand. To address these issues, Edp rebuilt the app in only two months using App Engine. The auto scaling functionality in App Engine means that their new app easily scales to meet peaks in demand and customers can now access their data even when Edp's back-end systems are under maintenance. The new app has delivered significant gains for Edp in terms of both performance and customer satisfaction. After Edp migrated it's customer service app to App Engine, the average page loading time decreased by almost 90 percent, and it's App Store reviews ratings jumped from 1.9-4.7 in just a couple of weeks, with downloads increasing as a result. There are many more examples of customers who have leveraged to Google Kubernetes Engine and App Engine as part of their digital transformation. Click on the links provided in the reading to learn about three customers in particular who increased developer velocity and provide amazing customer experiences.




https://cloud.google.com/customers/arcules/
https://cloud.google.com/customers/forbes/
https://cloud.google.com/customers/zulily/

For more customer stories, check out: cloud.google.com/customers




3.1 Overview

Introduction
Welcome to Module 3, The Value of APIs. What are they? I'll explain in just a moment. So far in this course, I've explored an organization's technical foundation in the Cloud. Many businesses have a variety of systems and applications, and for traditional enterprises, many of those systems and applications were built on-premises. For traditional companies, legacy systems and applications are complex, expensive to maintain, and do not provide the speed and scale required to deliver seamless digital experiences that consumers now expect. When it comes to digital transformation, companies typically have the following three primary goals; modernize IT systems such as compute solutions, modernize applications so they can remain relevant in today's Cloud era, and thirdly, leverage application programming interfaces or APIs to unlock and create value for customers. In this module, I'll explore how APIs are a tool for both digital integration and digital transformation. I'll begin by defining legacy systems and identify why they struggle to meet the demands of the digital age. Then I'll define APIs and how they can modernize legacy systems. Next, I'll explore examples of how APIs create new business value. Finally, I'll look at Apigee, a Google Cloud solution for developing and managing APIs. Let's jump in.




Legacy system challenges
Many enterprise decision-makers and IT admins are constantly choosing between maintaining legacy systems and developing new and innovative projects. Why is this often a trade-off for businesses? A legacy system is outdated computing software and/or hardware that is still in use. The legacy system is mission-critical, but often not equipped to deliver new services or upgrades at the speed and scale that users expect. Worse, a legacy system often cannot connect to new systems. Common examples of legacy systems include HR or employee management systems, banking systems, databases, data warehouses, data lakes, or systems design for government operations. These are all different systems to store and manage data. All of these systems, whether they're on-premises or in a private or multi-cloud or hybrid cloud environment are valuable to the business because they hold a large amount of data. But unlocking the value of that data is challenging. Why? I'll tell you. First, legacy systems weren't developed to support the implementation and adoption of modern technologies such as the cloud or the internet of things, or mobile applications. Second, they were developed for a time when data was shared in batches or at specific time intervals. This means that legacy systems are not designed to serve real-time data as it's expected in today's digital world. As a result, legacy systems tend to hold organizations back from using digital technologies to innovate or improve IT efficiency. Naturally modernizing IT infrastructure is central to digital transformation so that businesses no longer have to choose between maintenance and innovation. This means they need a well-designed integration strategy that leverages application programming interfaces or APIs. In the next video, I'll explain what APIs are and how they can be used to modernize legacy systems in more detail.




3.2 Leveraging APIs

How APIs can modernize legacy systems
In the previous video, I shared some challenges of legacy systems and mentioned that APIs are a way to solve for those challenges. But what is an API exactly and how can you use it to modernize your infrastructure? An API is a piece of software that connects different applications and enables information to flow between systems. Ultimately, APIs enable integration between systems so businesses can unlock value and create new services. They do this by exposing data in a way that protects the integrity of the legacy systems and enable secure and governed access to the underlying data. This allows organizations with older systems to adapt to modern business needs and more importantly, to quickly adopt new technologies and platforms. APIs enable businesses to unlock value without re-architecting all of those legacy applications. But it's important to remember that legacy modernization is not the end goal, rather it's a way to build long-term flexibility so an organization can meet evolving IT needs and better service customers. Customers expect real-time seamless experiences across platforms. Businesses now have the opportunity to digitize experiences throughout their value chain to meet their customer's expectations. Consider a traditional retail bank. They have lots of legacy systems that have valuable data. In the past decade, cloud-native start-up banks have entered the market and traditional banks must urgently adapt to a changing market. As a result, they want to provide customers with a connected digital experience through mobile banking. The app needs to show up-to-date account balance as soon as you open the application. The data that provides that information is stored in a legacy database. To connect that database to the end-user application, the bank creates an API that allows information to flow between the application and the legacy database seamlessly and securely. So how does this process work? The web or mobile apps are built by internal enterprise developers or by external third-party companies. APIs are built and managed by the API team within the enterprise. App developers leverage those APIs to integrate with backend services and other service endpoints. APIs are therefore central to any businesses' digital transformation strategy. They enable faster innovation so organizations can bring new services to market quickly and create new business value. I'll explore this in the next video.




Using APIs to create new business value
API started as a tool to facilitate access and integration. They still serve those functions. But now APIs can do so much more. When organizations start to think about and build an API first architecture, they can build new digital ecosystems and create new business value. A digital ecosystem is a group of interconnected companies and products. This includes vendors, third-party suppliers, customers and applications, just to name a few. A robust, well-connected and multifaceted digital ecosystem enables businesses to create and monetize new digital experiences. The more you know about your customers, the better you're able to offer a truly integrated end-to-end digital experience. The more services you have in your ecosystem to connect to those customers, the more you learn about them. APIs are not only a longstanding integration technology, they are the fundamental building blocks of digital transformation. Let's look at Walgreens, for example, a large organization based in the US with thousands of brick and mortar stores. Their business includes a range of products and services from pharmacy to photo printing to food and drink. Over the past decade, they embraced a culture of innovation and APIs were a key accelerator in their digital transformation. For example, one service they historically provided was photo development from film. With the advent of digital cameras and then smart phones, consumer needs changed. People weren't bringing in film to be developed anymore. So Walgreens asked themselves, how do we reengage smartphone users with photo printing? Walgreens built the photo printing API. This provided a photo of experience that allows developers who are building smartphone apps to connect to Walgreens to print out photos. Instead of only having create, edit, and share buttons for what an end-user can do with their photo, there's also a print button that connects to any Walgreens store. In this model, Walgreens is partnering with the developer community to serve customers in a new way. The developers are critical part of its digital ecosystem. Walgreens is therefore earning revenue from the developers, as well as the revenue from the customers who come in to print their photos. Walgreens treated the API as a product, not just a tool for integration. That API has created an entirely new revenue stream and enabled the photo printing line of business to return to being profitable. But Walgreens didn't stop there. Almost immediately after it launched the photo printing API, developers in its ecosystem approached Walgreens with ideas for other types of products they wanted to provide their customers, such as photo books, cards, and Canvas cards. By investing in its digital ecosystem and engaging developers, Walgreens was able to further monetize it's API and create new services to meet the demand. Another business that has embraced that API strategy to create new value is Monex. Monex is a global technology-based retail financial service provider. Their mission is to provide investors with the best financial services and liberal access to capital markets. They couldn't update their back-end systems quick enough to deploy new services or even modify existing ones. They developed an API to save them time and simplified processes of developing new products and services. With the API they're no longer constrained by their legacy systems. Instead, they can develop services and smartphone apps more rapidly. They've unlocked value from their existing backend services using an API and are able to provide more seamless digital experiences for their customers. But that's not all. They decided to publish their API for everyone in the financial technology business that is developing new apps. This has transformed how their partner financial technology firms display customer portfolio balances in their apps, improving security and performance for their business partners. It has also placed Monex at the center of the financial technology ecosystem. While they initially had a team of developers who worked on the API program, they quickly realized that it on-premises API Gateway wouldn't enable them to scale the program at the pace or at the performance they hoped for with the required built-in security. They turned to Apigee, an API management platform. But hang on. What is Apigee and what can it do? Check out the next video to learn more.




3.3 Google Cloud Solutions

Apigee
In an earlier video, I presented some of the common challenges that legacy systems pose for organizations. As companies adopt Cloud technology to meet their business needs, there's a widening gap between modern applications and legacy systems. In this video, I'll begin by providing an overview of the infrastructure and application development gaps. Then list how Apigee addresses these gaps and finally, highlight a customer success story. Legacy systems like CRMs, ERPs, SOAs, databases, data warehouses, and data lakes provide businesses data. But don't provide features and capabilities at the rate of change demanded by today's users. Modern applications, on the other hand, provide connected experiences and can be rapidly updated to meet user demands. Applications that provide these connected experiences to end-users must be able to do so securely and at scale. Developers, therefore, need to manage the entire application life cycle. Connect to different backend systems, including the legacy ones, and be able to track and analyze the interactions between consumers, data, and service producers. Many businesses started with a small team of developers who were responsible for creating APIs, specifically for modernizing legacy systems and for creating modern applications. But as the company's digital ecosystem becomes more complex, the required time and effort to manage hundreds of APIs securely and at scale becomes costly. That's where Apigee comes in. The Apigee platform includes an API services layer that provides the runtime API gateway functionality. The Apigee platform includes developer services. This means that a developer can access a portal to utilize your APIs for their projects. They can also register their applications. Measuring and tracking the performance of APIs is a critical component in API management. Let's take a look at a customer that used Apigee as part of their digital transformation strategy. Chile's Ministry of Health faced an enormous challenge. Their health care services and medical facilities lacked connectivity and interoperability between their respective systems. This meant that healthcare professionals struggle to access comprehensive medical records and couldn't utilize new technologies to provide better patient care. Previous integration attempts failed because they were so time-consuming and expensive. They embrace that API first architecture to support some key programs. Including a national program to join disconnected health care centers and apply to digitize all clinic and administrative processes. The Apigee platform has been the accelerator for the entire program providing visibility and controls that make APIs easier to manage. The Ministry of Health was able to reduce costs by sharing information, eliminating delays, and reducing the duplication of medical tests. If you'd like to learn more about Chile's Ministry of Health Transformation, check out the link on the screen.
https://blog.google/products/google-cloud/transforming-chiles-health-sector-connectivity/




Summary
And we've reached the end, congratulations for completing this course. We've covered a lot and examined some highly technical content. So let's recap the key points, businesses need to provide seamless digital experiences to remain relevant in the cloud era. Legacy systems and applications fail to deliver the real time speed and scale needed to provide those experiences. Modernizing IT infrastructure is essentially a process that breaks down monolithic systems and applications into the smallest units that use a shared pool of resources. As a result, when new feature requests come in or changes are needed, each unit can be updated and deployed easily and at scale. Modernization also involves handing over more and more of the back end maintenance to cloud providers. This means your IT teams can focus on innovating and delivering business value instead of wasting time on intensive maintenance. Application development and deployment is an important part of a company's modernization strategy. Containers in particular enable our development teams to be more agile and bring fresh experiences to customers quickly with minimal downtime. Application programming interfaces or APIs often sit in between legacy systems and modern applications and enable information that was previously inaccessible to flow between them. APIs can also be leveraged to create and monetize new services to help your organization uncover new business value and meet the demands of customers. In this course, I also shared the common pathways to digital transformation and which google cloud solutions can help. We encourage you to learn more about what this means for your business operations. Refer to Understanding Google Cloud Security and Operations to learn more about cost management, resource monitoring and how to keep your data and systems protected in the cloud. For the full google cloud course catalog, checkout cloud.google.com/training, and if you signed up for the learning path, be sure to complete all four courses to receive credit. And that's it for this course, now that you know more about providing seamless connected digital experiences, start thinking about the new services you can develop for your customers. We can't wait to find out what you come up with.




































#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#




Understanding Google Cloud Security and Operations

Course Introduction
By now you've probably heard that cloud technology enables organizations to unlock business value and create new customer experiences, but embracing cloud effectively requires a change in mindset in business operations. One important change is how tech and non-tech-adjacent teams operate. This is because traditional team structures and approaches for managing costs, security, and resources struggle to provide the speed and scale necessary to thrive in the cloud. So in this course, I'll examine cost management, security and operations in the cloud, and explore best practices for digital transformation in each area. In module 1, I focus on cost management in the cloud. Traditional IT expenditure models required companies to spend a substantial amount of money up front to up and maintain on-premise IT infrastructure. In the cloud, business maintain some or none of their own infrastructure. Instead, they purchase IT services from a cloud provider on an ongoing basis. I'll cover what this means for cost planning and management. In module 2, I examine security in the cloud. Any business that accesses, stores, or uses data is responsible for protecting and securing that data. In the cloud, the responsibility of data security is shared between the cloud provider and the business. In module 3, I cover how IT teams and business leaders need to rethink IT resource management in the cloud. This includes balancing service availability and reliability with agile application development, as well as evolving a team structure to better service customers. I'll then close the course by summarizing the key points and offer additional resources for you to continue your learning. Throughout the course, you'll have an opportunity to test your knowledge by completing the graded assessments. You'll need to pass these assessments to receive credit for the course. Let's begin.




1.1 Overview
Hello and welcome to the first module Financial Governance in the Cloud. Embracing cloud technology enables any business to unlock value, create new services, and reimagine its business model to better achieve its mission. However, leveraging cloud technology either for business improvements or for large-scale transformation doesn't come without some challenges. In fact, one of the common pain points many organizations face regardless of which cloud provider they use is managing cloud costs. This includes anything from planning and budgeting for cloud resources to being able to monitor and control monthly spend to optimizing it as needed to accurately forecasting future costs. For large organizations especially, the transition from predictable capital expenditures for building and maintaining their IT infrastructure to agile operating expenditures using cloud resources requires process and organizational changes. Yesterday's solutions for predicting and controlling costs don't work well in the cloud era. Yes, it's true that cloud technology can provide organizations with the means to make more dynamic decisions and accelerate innovation, but managing cloud costs requires vigilance and real-time monitoring in parallel. In fact, because almost anyone can now access cloud resources on demand, managing IT infrastructure costs no longer sits mainly with the finance team. Instead, it involves more people across multiple teams. So you might be the person responsible for IT budgeting. You might be responsible for spinning up cloud resources. Or you might be building your business plan for a transformational project using cloud resources. Whatever your role, understanding how using cloud technology affects your business from a cost perspective will help you maximize the business value your organization gains from using the cloud. Let's go over what we'll cover in this module. I'll explain the core challenges organizations face when managing cloud costs and propose a solution through three lenses. Next, I'll explain how to think about the total cost of ownership when using cloud services. Then I'll unpack core Google Cloud cost management and concepts, and finally I'll describe some best practices for effective cloud cost management. Let's get started.




1.2 Cloud Cost Management

Fundamentals of cloud cost management
For organizations that build and deploy applications on premises, there's a heavy emphasis on capital expenditure, or CapEx, to setup and maintain their IT infrastructure. For example, setting up a data center requires planning the necessary amount of physical space, servers and other hardware, then purchasing it and hoping that it's just enough.

It's a careful balancing act between under-purchasing and over-purchasing so that a business doesn't end up with unserved demand or wasted capacity. The operating expenditure would include, for example, the utility bill for keeping the space full and salaries for security personnel. In contrast, when an organization migrates or builds and deploys applications using cloud services, there's a greater emphasis on operational expenditures, or opex. They're paying for what they need, when they need it. But what does this focus on operational expenditures actually mean in practice? Well, budgeting is no longer a one-time operational process completed annually. Also, because of the variable nature of cloud resources and their costs, spending must be monitored and controlled on an ongoing basis. Infrastructure procurement has radically changed too. Now given the right permissions, almost any employee can spend up resources in seconds. Too often, though, the very accessibility that makes cloud services attractive leads to reduced control and significant overspending. Often, this is unintentional. An engineer spins up a virtual machine and then forgets to shut it down, or makes a seemingly minor change to an application that then leads to spiraling data storage costs due to a bug in their code. The ease of access to cloud resources brings with it the need for more precise, real time control of what is being consumed. Having control can mean the difference between peace of mind, and spiraling costs that lead to budget overruns. If this is something you're concerned about, you're not alone. We've learned that IT executives found that unpredictable costs and lack of visibility and transparency into the cloud usage are among the top pain points when managing cloud environments. Concern over lack of predictability was also a top pain point for IT and finance professionals in a recent Google study. And these concerns are valid. When cloud usage isn't effectively controlled, it can lead to inefficiencies. According to a RightScale study from Flexera, businesses surveyed estimated they were wasting 27% of their cloud spend. What Flexera found was that businesses were actually wasting 35% of their cloud spend. 8% over their estimates. This suggests that the methods cloud users apply for managing their cloud spend are ineffective. Moreover, they're underestimating their cloud financial waste. Keeping costs within budget and limiting surprises were among the most critical cost management tasks identified in Google's research. The people involved in the cost management process will be key to helping you and your organization keep costs under control. Now if you're working at a small organization, there may be one person or team responsible for managing all aspects of a cloud infrastructure and associated finance. From budgeting to procurement, tracking observation and so on. If you're in a larger organization, there are probably more people involved across multiple functions, including finance, technology, and line of business roles. For these organizations, technology and line of business teams are often the ones using cloud resources, but they don't necessarily factor costs into their decision making. And similarly, finance teams care about controlling cloud costs, but they may struggle to understand or keep up with cloud spend on a daily, weekly, or monthly basis. So how do you solve this problem? Let's look at the solution through three lenses. People, process, and technology. By people, I'm referring to the various roles involved in managing cloud costs. Again, if you're working in a small organization, one person may have multiple roles. For large organizations, though, the finance team needs to take on a financial planning and advisory role. They need to understand their organization's business priorities, then make data-driven decisions around cloud spending. Working in close collaboration with technology and line of business teams. These teams can then advise on how cloud resources are currently being used to meet the organization's overall business strategy and what additional resources might be needed throughout the upcoming year. To manage cloud costs effectively, a partnership across finance, technology, and business functions is required. This partnership may already exist, or it may take the form of centralized hub, such as a cloud center of excellence. The central team would consist of several experts who ensure that best practices are in place across the organization and there's visibility into the ongoing cloud spend.

The centralized group would also be able to make real time decisions and discuss trade-offs when spending is higher than planned. When it comes to cloud cost management, consider the following questions: What cloud resources are being used and by whom? What are the associated resource costs? How do these costs measure against the broader business strategy?

With Cloud, an organization can and should monitor and analyze its cloud usage and cost on a daily or weekly basis. This will often be done by someone on the technical team. Then on a weekly or monthly basis, the finance team can look at the results, charge back the costs through the appropriate teams, and determine whether any changes are needed to ensure that the organization's cloud spend is optimized.

Having a culture of accountability in place across teams helps organizations spot waste, quickly take action to eliminate it, and ensure they're maximizing their cloud investment. It will also help drive cross-group collaboration across technology, finance, and business teams to make sure their cloud spend aligns with broader business objectives. Finally, Google Cloud brings its own native tools that help organizations monitor and manage their costs. In fact, these tools can enable organizations to gain greater visibility, drive a culture of accountability for cloud spending across the organization, control costs to reduce to risks of overspending, and provide intelligent recommendations to optimize cost and usage. We'll talk more about some of these benefits in a later video. For now, just remember: The variable nature of cloud costs impacts people, processes, and technology.

As an organization adapts, it'll need a core team across technology, finance, and business functions to work together to make decisions in real time. Through this close collaboration, they'll be able to control and optimize cloud costs on an ongoing basis. Up next, we'll discuss the factors that make up the total costs of ownership when using cloud services.




Total cost of ownership
Whether an organization is moving to the cloud for the first time or moving from a single cloud provider to multiple providers, how you calculate the total cost of ownership of the IT infrastructure will vary. In IT, the total cost of ownership refers to a comprehensive assessment of all of the layers within the infrastructure and other associated costs across the business over time. This includes acquiring hardware and software, management and support, communications, and user expenses, and the cost of service downtime, training, and other productivity losses. Let me use an analogy to explain this. Let's say there are two main ways for your business to get pizza. One, you make it. We'll call that on-premises pizza. Two, Google makes it. We'll call that Google Cloud pizza. Now, if you're making on-premises pizzas, you need to pay for the ingredients, and you also need to pay for the tools to make it, the gas to cook it, maintenance of the kitchen, kitchen staff, and so on. And don't forget all the time someone has to invest in preparing it. I'm sure you could easily draw up a long list of things that add to the total cost here. If you change from on-premises pizzas to Google Cloud pizzas, suddenly you'd be responsible for a lot less. You would just choose the pizza you want, maybe the custom toppings, order it, and eat. And if you need more pizzas one day, you just order more and pay for what you order. So bringing this back to the real world, historically when companies spent a substantial amount of money upfront to set up their IT infrastructure, the capital expenditure would include paying for data center space and associated costs, such as power and cooling, storage systems, networking, hardware, software, and security systems. The total cost of ownership, or TCO, in this case, would be the cost of setting up, managing, controlling, and optimizing every layer of the stack, in addition to the personnel required and skilled workers. Before the cloud, IT CapEx costs were often managed centrally. For accounting teams, a data center for example would be treated as a property or asset with a lifespan of three to ten years and with a depreciation value for tax purposes. When organizations run their business using public cloud services, much of their capital expenditure no longer applies. Instead, there's a shift towards a pay-as-you-go OpEx model. This shift opens up room for their technology teams to focus more on building innovative solutions in the cloud instead of maintaining the existing infrastructure. It's worth mentioning that some organizations may choose to keep some of their business running on-premises and some running on public cloud. The total cost of ownership for them would be more complex. The total cost of ownership when using cloud technology is not just about assessing cost savings. You also need to think about the value you gain over time. The obvious value is that cloud absorbs the effort that organizations traditionally put into hosting their applications or data on-premises, allowing them to shift focus from maintaining status quo to higher value work. There's also indirect value. The cloud improves efficiency, reliability, and security, enabling greater productivity and innovation for businesses. So as we've talked about in this video, assessing IT total cost of ownership can vary depending on an organization's cloud adoption goals and can continue to evolve over time. Google Cloud provides tools to help monitor and control costs and maximize value. Learn more about those in the next video.




1.3 Google Cloud best practices

Best practices for managing Google Cloud costs
In an earlier video, I explained that adopting cloud technology will inevitably require fundamental changes for cost management. We explored these changes through three lenses: people, process, and technology. I then highlighted that businesses will need to monitor and control their cloud service cost on an ongoing basis. In this video, I'll present three best practices and advantages for using available Google Cloud tools for ongoing cost management. First, organizations need to identify the individual or team that manage costs. If it's a team, it should ideally be a mix of IT managers and financial controllers. Second, it's important to understand what kind information can be found in an invoice versus cost management tools. An invoice and Google Cloud tools are not the same thing, so be sure to reference them accordingly. Third, organizations can use cost management tools for accountability and to gain visibility, control, and intelligence. I already covered the first point in an earlier video, so I won't spend time repeating it here. Let's explore the next two in more detail, starting with distinguishing between the function of an invoice and cost management tools. An invoice simply tells you how much you're spending over a period of time. Seldom though is an organization interested only in how much they spend. They want to know why they spent that much. Cost management tools like the ones built into the Cloud console are more effective for answering the why. The tools will help financial controllers and IT leaders, for instance, get more granular data, find trends, and identify actions to take to control or optimize costs. All right. Now that I've briefly spoken about the importance of distinguishing between an invoice and cost management tools, let's look at the third best practice. This focuses on how you can use these tools to get more out of the Cloud. Google Cloud believes in supporting organizations by providing strong financial governance tools that makes it easier for customers to align their strategic priorities with their Cloud usage. So the goals of the cost management tools are to provide visibility, accountability, control, and intelligence so that businesses can scale in the Cloud with confidence. Let's examine each of these terms.

Before organizations can optimize their cloud costs, they first need to understand what they're currently spending, whether there are any trends, and what their forecasted costs are. This means they need visibility into their cloud costs. An organization needs to start capturing what Cloud resources are being used, by whom, for what purpose, and at what costs. The organization also needs to think about who is going to be responsible for monitoring this, who needs to be involved in managing costs, and how they will communicate the results or report on spending on an ongoing basis. We've already discussed the importance of reviewing business priorities and establishing partnership between financial and IT teams, perhaps through a centralized team. It's also important to set up the cadence and format for ongoing communication with key cloud stakeholders. Having this plan outlined up front will help ensure that managing costs isn't an afterthought. But how will the central team monitor current cost trends and identify areas of waste that could be improved? They can use Google Cloud built-in reporting tools and create custom dashboards to gain greater visibility into their costs. The team should review these reports at least weekly. Another useful tool is the pricing calculator. This allows an organization to see how changing usage will affect their costs. You can find the pricing calculator by following the link on the screen: cloud.google.com/products/calculator. Because Cloud spending is decentralized and variable, it's important to establish a culture of accountability for costs across the organization. Google Cloud offers flexible options to organize resources and allocate costs to individual departments and teams. Defining clear ownership for projects and sharing cost views with the departments and teams that are using Cloud resources will help us establish this accountability culture and more responsible spending. This team should use these tools to regularly identify and report on cost inefficiencies. In addition to making teams accountable for their spending, Google Cloud financial governance policies and permissions make it easy to control who has the ability to spend and view costs across your organization. Organizations should also have precise permissions in place to ensure that only authorized individuals in an organization have the power to deploy cloud resources. Also creating budgets and alerts to notify key stakeholders when spending is getting off track, it is an important practice to keep costs under control. With programmatic budget notifications, organizations can automate actions based on the unique requirements for your organization or industry. For example, if your organization relies on grants, you could cap spending for a specific resource so you don't unintentionally exceed the total grant amount. And finally, organizations can make smart spending decisions with intelligent recommendations delivered by Google Cloud. These are tailored to each organization and help optimize usage, save time on management, and minimize costs. An organization can see these recommendations which can easily be applied for immediate cost savings and greater efficiency.

We've now covered all three best practices. You might be in a position to implement some of these best practices yourself. If not, pass them on to the relevant stakeholders within your organization. In the next module, I'll explore security in the cloud and how organizations who embrace cloud need to also embrace a new model for protecting their business.




2.1 Overview

Introduction
Organizations that adopt cloud technology will inevitably need to fundamentally change some of their business operations. The first of these is about financial governance. How leaders plan, set up, manage, and control their IT costs. I covered this in the last module. The second fundamental operational shift is about security. From securing physical data centers to a global network, to private user data across the globe. In this module, I'll examine a new cloud-first security model and explain how it differs from traditional on-premises IT security models. I'll start by defining a few fundamental terms related to security in the cloud. Then I'll discuss today's top cybersecurity challenges and the most common threats to data privacy and security. Next, I'll use the shared responsibility model to illustrate how you and your organization can maintain data security and compliance on an ongoing basis. I'll then look at how Google Cloud equips organizations to control and manage access to data and resources through cloud identity and through an understanding of the resource hierarchy. Having a robust security program will be critical to any organization's clouded adoption journey. So let's get started.




Fundamental terms: Privacy, security, compliance and availability
Privacy, security, compliance, and availability, these terms often get bundled together or used interchangeably because there's some overlap. It's important, though, to distinguish between them, especially when evaluating cloud service providers. I'll start with privacy. Privacy in the context of cloud technology refers to the data an organization or an individual has access to and who they can share that data with. Imagine that you have an important letter or document. It contains private information, so you lock it away in a drawer in your house. You also keep the key with you, so you can be sure no one else has access to it unless you give them permission. With traditional technology, an organization would store that letter, or their private data, on-premises where it feels safe because they generally know where it is and trust that the letter will be kept private. Storing data in the cloud is different in that it's more like taking that private information and keeping it in a commercial storage facility. It's locked away just as before, but now you've relinquished some of the control to someone else. That facility will likely have better security controls than you could provide yourself. Now, storing your data in a facility you don't own or completely control might be daunting. You might be wondering, how can I be sure no one else is accessing my data? Or what could a facility manager, for example, do with my data? And the answer is, when moving your data to the cloud, the facility and its employees only store or process your data. The data itself remains private. I'll cover more details about how Google Cloud ensures that customer data remains private a little later in this module. Let's move on to the next term. Security in the cloud refers to the policies, procedures, and controls put in place to keep data safe. If we go back to the letter analogy, the lock on the drawer is a security measure. Why is this important? If an organization regularly handles customer data, for example, it has a responsibility to its customers to ensure that customer private data is protected from external threats. Data breaches put an organization's reputation at risk, and in today's fast-paced global economy, reputation is everything. So business leaders want to trust that only authorized people have access to sensitive company information. Compliance takes data security one step further. It's about meeting standards set by a third party. This third party might be a regulatory authority, or it might be an international standards organization. Compliance is especially important in highly regulated industries, such as healthcare or banking, where there's an abundance of sensitive data. In these cases, an organization's approach to data security needs to meet any requirements set forth by the relevant standards or regulatory bodies in its region or industry. The next term is availability. IT teams want to prevent unauthorized access to data, yet still make sure the data is available when needed. This describes the availability or reliability of a service. Availability refers to how much time the cloud service provider guarantees that your data and services are up and running or accessible. The availability of a service is typically documented as a percent of time per year, for example, 99.999% or 5/9. In this example, services would only be unavailable for five minutes in a year. To assess the availability of a service, you might ask, for example, does the system work? Am I confident that I can access my files anytime I need to, day or night? Or will I not have access due to system downtime? Whether you're working in healthcare, banking, retail, or even education, it's critical to understand how you and your cloud provider can work together to keep your organization's data private, secure, and compliant while maintaining service availability and subsequently, reliability. If you're using or plan to use Google Cloud products and services, Google Cloud's commitment to helping you keep your data secure and private is as follows: You own your data, not Google. That means that your data is your data, period. Google doesn't sell customer data to third parties, and Google Cloud does not use your data for advertising purposes. Google Cloud only processes a customer's data according to the specific instructions it receives. Next, all customer data is encrypted by default. Google Cloud was the first cloud provider to encrypt all customer data at rest by default without customers needing to do anything. Google Cloud guards against insider access to your data. Insiders are only able to access customer data with their permission or in very specific scenarios with specific instructions. Google never gives any government entity backdoor, or unlawful, access to data or to Google servers that store data. Google rejects government requests that are invalid and regularly publishes a transparency report detailing government requests. And finally, our privacy practices are audited against international standards. This includes standards like ISO 27-0-17, which covers the protection of personally identifiable information, or PII, in cloud services. All right, we've covered the definitions for key terms and Google Cloud's commitments to securing customer data. In the next video, let's move on to today's cybersecurity challenges.




Today‚Äôs top cyber security challenges
We've all seen headlines like these. New worries for CEOs, a career ending cyberattack, cost of a retail data breach, $179 million for Home Depot, target to pay $18.5 million for 2013 data breach that affected 41 million customers. Cyberattacks are bigger than ever, and today they can come from pretty much anywhere. Many groups, including those representing a government body, might use sophisticated methods to gain access to an organization's data. These attacks have become possible because we live online. This means that almost every organization is digitally connected to their customers, partners, and even their employees globally. Although there are many advantages to digitally connected global systems, these systemic and inherent complexities create new risks. Let me explain. Traditional on-premise systems or company own datacenters generally rely on perimeter-based security approach. That means the boundary around all of their data is protected by a firewall, for example, along with other security features. Once someone is inside that security perimeter, they're deemed trustworthy and therefore have access to everything. If you think of this in the same way you think about securing your house, all you have to worry about is the perimeter. Maybe you have a gate with a lock as an added measure, you also secure all the doors and windows with locks and security keys are codes. Now imagine what would happen if you remove the roof, the doors, and the windows? Every object in person would be exposed. In this case, true security will only be achieved when each object in person is individually protected. That's exactly where we are today. We're in the Internet of things era where everything is connected through sensors that collect data, your phone, your car, and even your credit card. Everything is a node on a network, and when everything is a node, each node becomes an entry point. What are the common cybersecurity threats in this case? The first threat is constant criminal attacks. This means that phishing attackers do research to gather information about you or anyone in your organization, for example, employees or students. They then craft highly targeted e-mails to trick these people into thinking that the messages are genuine. Anyone in your organization can then be scammed into downloading malicious attachments, giving up their password, or sharing sensitive data. The second threat is physical damage. This means that organizations can still be responsible for data losses even when there is damage to the physical hardware. There are power losses or natural disasters such as floods, fires, and earthquakes. The third threat is malware, viruses and ransomware attacks. Data can be lost, damaged, or destroyed by viruses or malware. Alternatively, a set of files can be rendered unavailable to its intended users via ransomware until the ransom amount is paid. A fourth risk is unsecured third-party systems. Although third-party systems are often used to address common business needs like finance, inventory, or account management, without adequate security measures and regular checks, these systems can pose a threat to data security. Finally, misconfiguration is another risk to an organization. While this can be due to a lack of expert knowledge, even export Cloud engineers can misconfigured systems. In fact, misconfiguration is the single biggest threat to cloud security according to both the NSA in a 2018 survey by cybersecurity insiders and Crowd Research Partners. This is why access should be limited following a least privilege zero trust model. At the rate that technology is changing, investing in the right expertise to assess, develop, implement, and maintain data security plans is essential for businesses to stay ahead of potential data security threats. As the wider technological Ecosystem evolves, so too are the threats, and businesses need to be ready. Now, how do you defend against security threats that can come from anywhere, at anytime from anyone? A perimeter security model is not enough anymore. In fact, you'll soon discover that leveraging Cloud technology for your organization can dramatically strengthen your organization's ability to secure a data against newer and more sophisticated threats. As data security becomes more complex and the right expertise becomes more scarce, a collaborative approach with your cloud provider is crucial to keeping your organization's data secure. Find out more in the next video.




2.2 Shared Responsibility Model

Shared Responsibility Model - Part I
When an organization manages its data and its own data centers, that organization is then responsible for all aspects of its security. The advantage of using cloud technology, on the other hand, is that the responsibility to secure data is shared between a business and the cloud provider. First, let's be clear about everyone's role in the shared model. When an organization adopts the cloud, the cloud service provider typically becomes the data processor. The organization is the data controller. While the cloud service provider manages the security of its infrastructure and its data centers, customers gain the benefits of their infrastructure's multiple built in security layers. This is referred to as defense in depth. The customer's core responsibility is to secure access to data while the cloud provider-- in this case, Google Cloud-- is responsible for securing the underlying infrastructure. It's important to think of this as a partnership between the customer and the cloud provider where their provider also offers tools and best practices to help customers with their portion of the responsibility. Everything Google has learned over the past 20 years has driven how it designs and secures its infrastructure, the same infrastructure that is used by Google's enterprise customers. Let's look at how Google as the data processor protects its infrastructure. Google implements a defense in depth approach to security as a secure foundation. As of early 2021, Google Cloud has over 124 points of presence, or POPs, worldwide, connecting 24 data center regions, and that's where the security efforts start. There is layer upon layer of security built into Google Cloud products and services. Let's look at Google Cloud's multi-layer approach more closely starting with hardware. Google designs its own servers, its storage, and its networking gear. In fact, it manufactures almost all of its own hardware, and third parties never see the overall process. The hardware is housed in these high security data centers that are located around the world.

New server builds have an embedded chip called Titan. Titan checks the machine for integrity every time it boots up. The next step is software. The Titan microcontroller continues to verify the operating systems and the rest of the deploy software stack. The server is not allowed onto the network, and it holds zero data until its health is confirmed. The next layer is storage. Storage is closely connected to the idea of data encryption at rest, so let's talk about what that means. Encryption at rest protects data when it's stored on physical media like a hard disk. And that's not all. All data at rest is also encrypted by default to help guard against unauthorized access. Let me explain how it works. When data is going to be stored on Google Cloud, it goes through the following process. First, it's broken into many pieces in memory. These pieces or chunks are encrypted with their own data encryption key or DEK. These data encryption keys are then encrypted a second time or wrapped, generating another key which we call a key encryption key or KEK. Encrypted chunks and wrapped encryption keys are distributed across Google's infrastructure. In the unlikely event that someone compromises any encryption key, they could only access one tiny piece of data, which, without all the other pieces, would be unreadable. Next is identity. Instead of relying on traditional perimeter approach to security,

Google Cloud operates a zero trust model. This means that every user and every machine that tries to access data or services must strongly authenticate at every stage for each file. Anyone accessing the cloud does so via a network. This is the next layer. Encryption in transit protects data as it moves across the network. The data in transit, that is, all data moving into and out of Google's infrastructure is encrypted in transit. Multi layers of defense are in place to help protect customers against network attacks like DDoS attacks.

The final layer is the operations layer. At Google, a global team of more than 900 security experts monitor the system 24 hours a day, 365 days a year. Their role is to detect attacks and other issues and respond to them. In addition to these multiple security layers, Google Cloud has an array of features and policies that its customers can use to control access to their data. The next video will cover more information about using these features and policies.




Shared Responsibility Model - Part II
In the previous video, I presented an overview of the shared responsibility model focusing on Google's responsibility. Now let's talk about your organization's responsibility in the model. In particular, what do IT teams need to do to protect their data in the cloud? First, it's important to note that the old ways of securing data may not work at all, and so the associated operations have to change. Continuing to do what you've always done might not keep your data secure enough in the cloud environment. Organizations need to reassess their existing security policies and practices and determine how to best use cloud products and solutions to maintain and potentially improve their security posture. Ultimately, IT teams need to control data access, maintain visibility, and be prepared for incidents. Let's break each of these down. First, IT teams need to have a complete understanding of who can access what data. Wherever possible, they need to establish granular access policies. In other words, they need to define who can do what and on what cloud resource. Remember that employees value convenience. This means they often do things without keeping the security of their data top of mind. They write down simple passwords, plug infected USB flash drives into their computers, and delay software updates to avoid painfully long reboots. IT teams can help employees get rid of some of these hassles. How? By enabling multi factor authentication to protect against phishing without degrading the user experience. Security keys are easy to use and complement password protection. A user can simply plug in a security key, and it serves as an extremely effective protection layer between employees and a potential phisher. In fact, since deploying security keys, Google itself has had zero Google workspace account hijackings.

Second, IT teams and business decision makers need to ensure that they have visibility into what's happening, who is accessing what data and when. Logging and monitoring tools are used here and many of them are natively provided as part of Google Cloud services. Finally, how do organizations manage a data breach? You hope it never happens, but you have to assume it will, and so it teams and business leaders need to have a plan in place to successfully deal with it. First, they need to know what's going on by developing situational awareness. Next, organizations need to have a culture that allows teams to work in stressful situations. Put simply, they need to create an open, blameless culture. And, finally, organizations need to be ready for what's coming. In other words, they need to maintain operational readiness. To enable customer IT teams, Google Cloud provides best practices, templates, products, and solutions for controlling data access, maintaining visibility, and preparing for breaches. Remember that keeping data safe requires a collaborative approach between the cloud provider and your organization. In the next two videos, I'll examine two ways that Google Cloud equips organizations to control access to data information, identity and access management, and resource hierarchies.




2.3 Google Cloud Solutions

Identity and access management
Cloud Identity is a Google Cloud solution that helps organizations control and manage access to resources in order to maintain the security and integrity of both data and systems. An identity access management policy or IAM policy is made of three parts: who can do what, and on which resource. The who part of an IAM policy can be a Google account, a Google group, a service account, or a Google workspace or Cloud Identity domain.

The "can do what" part is identified by IAM role. There are three kinds of roles in Cloud IAM: primitive, predefined, and custom. Let's talk about each in turn. Primitive roles are owner, editor, and viewer and are broad. An account administrator, for instance, would apply primitive roles to a Google Cloud project, thereby affecting access to all resources in that project. If you're a viewer of a given resource, you can examine it but not change its state. If you're an editor, you can do everything a viewer can do plus change its state. And if you're an owner, you can do everything an editor can do, plus manage roles and permissions on the resource. Be mindful, though, if several people are working together on a project that contains sensitive data. Assigning primitive roles is not the most effective approach to security. Fortunately, Cloud IAM provides more granular roles. Now, if you're not sure what permissions to grant specific users, Google Cloud services offer their own set of predefined roles that align with typical responsibilities of people using those services. Each role is a collection of permissions. The Cloud service defines where those roles can be applied. For example, you might be familiar with Compute Engine, which offers virtual machines as a service. Compute Engine also provides a set of predefined roles that can be applied to Compute Engine resources in a given project, a given folder, or an entire organization. As another example, consider Cloud Bigtable, which is a managed database service. Cloud Bigtable offers roles that can be applied across an entire organization, to a particular project, or even to individual big table database instances.

What if you need something even more granular? That's what custom roles permit. Google Cloud recommends using a least privileged model in which each person in your organization is given the minimal amount of privilege needed to do their job. So for example, maybe you want to define a role to allow some users to stop and start Compute Engine virtual machines, but not to reconfigure them. Custom roles allow you to do that. Organizations that use custom roles need to manage the permissions that make them up. Some companies decide they'd rather stick with the predefined roles because they don't want to spend time managing custom ones. If you're thinking that assigning and managing all these permissions is going to be time consuming and difficult, don't worry. An organization can easily map job functions within the organization to specific groups--for example, a sales group for the sales team, or a marketing group for the marketing team. Each group can then be given specific roles for specific resources. This means that users get access only to what they need to do their job. And admins can grant default permissions to entire groups of users. Using permissions helps keep your organization secure when you're using Google Cloud products and services. Now that I've covered Cloud Identity access management, let's examine the resource hierarchy.




Resource hierarchy
In the last video, I talked about Cloud Identity, a tool for controlling and managing user access permissions. Another facet to controlling and managing access is tied to the resource hierarchy, or in other words, what resources users can access. To understand resource hierarchy, it's helpful to use an analogy. Think back to a project you managed at work that involved a group of peers. It doesn't have to be technology related. If you set up the project, you probably planned out what resources you needed, and who should be involved in the project as well. You organized your project files into folders so that you and your team could easily retrieve relevant information throughout the project lifecycle. Managing files, folders, and resources for projects is very similar to how teams would use and manage Google Cloud services. First, let's start with the project itself. In the Cloud environment, a project is the basis for enabling and using Google Cloud capabilities, like managing API's, enabling billing, adding and removing collaborators, and enabling other Google or Alphabet services. Next, again, in a Cloud environment, a resource is any Google Cloud service, such as Compute Engine and BigQuery. So any resources consumed by your project, for example, virtual machines, Cloud Storage buckets, or BigQuery tables are connected to the project in the hierarchy. Businesses usually have more than one Cloud project running, so projects can be organized into folders. A folder can contain projects, other folders, or combination of both. This means projects can be grouped into a hierarchy. So when I say resource hierarchy, I'm referring to the way your IT team can organize your business's Google Cloud environment, and how that service structure maps to your organization's actual structure. For example, by teams or by projects or by both. With a resource hierarchy, IT teams can manage access and permissions for groups of related resources. Starting from the top, everything managed in Google Cloud is under a domain and an organization. Think of this like an umbrella. The domain is handled through Cloud Identity, and helps manage user profiles. The organization is managed through the Cloud Console, and lets administrator see and control Google Cloud resources and permissions. Projects belong to the organization rather than the user who created them. As I mentioned earlier, projects are used for grouping Google Cloud resources, like Cloud Storage buckets. A project can exist under a folder, so it can be grouped logically to match a company's actual organizational structure. It can also inherit permissions from any folders above it, as well as from the organization at the top, making it easy to set organization-wide rules and policies that cascade down and are enforced throughout the hierarchy. Folders and projects can have permissions that let the administrator control who can create, edit, or just view resources inside of them. For example, you can set up specific users to be project creators, so they can create new projects and set up other users to be project viewers so they can see what resources are being used and view the cost for individual projects. Their permissions and structure are flexible, so a business can organize the hierarchy to meet its needs. Two other roles that are important for resource management are Cloud Billing accounts and payment profiles. Cloud Billing accounts live under the organization and track any charges for associated projects. Cloud Billing account users can associate projects and see spend, while Cloud Billing account administrators are able to unlink projects, set budgets, and contact billing support.

Google recommends sticking to a single Cloud Billing account per organization, and making sure that only admins can create new Cloud Billing accounts. This helps manage spend more efficiently and provides greater visibility into Cloud usage, as there's a single point of resource management. The payments profile is a Google-level resource that sits outside of Google Cloud, and is used to pay for all Google services such as Google Cloud, Google ads, or Chrome licenses. In the next module, I'll cover the third operational shift by examining best practices for monitoring Cloud services and resources.




3.1 Overview

Introduction
Cloud demands that businesses radically transform how they operate to thrive in the cloud era. And up to this point in the course, I've explored cost management and cybersecurity as two focus areas for operational change. The third operational shift focuses on how an organization monitors its IT services, whether on premises or in the cloud, to deliver optimal customer experiences. In this module, I'll start by exploring IT operational challenges, specifically how IT teams are traditionally structured, and why that structure prevents organizations from quickly delivering updates to services or fresh customer experiences. Next, I'll define what DevOps and site reliability engineering are, and then use them as a framework for IT operational changes. Finally, I'll examine Google Cloud resource monitoring tools and how they help organizations maintain control and visibility of their cloud environment. Let's get started.




IT development and operations challenges
Have you ever logged on to a website and come across one of these messages? "Page under construction," or "503 service unavailable," or something similar? The message may be the result of a planned maintenance. In this case, the company wants to release updates to their website, so they need to take their service offline while changes are being implemented. Alternatively, the message on the screen may be the result of an unexpected system failure, and engineers are trying to fix the problem as quickly as possible. Unexpected or prolonged downtime can be irritating for end users and costly for businesses, including from loss of customers. For this reason, IT leaders want to avoid service downtime. But service downtime is unavoidable for IT teams, and it's also a source of two operational challenges. First, developers are expected to continuously improve customer facing services. To do this effectively, they have to schedule system downtimes on a monthly, quarterly, or yearly basis. This allows them to release new updates at a regular cadence. Even though system updates are typically scheduled outside regular business hours, in today's global digital economy, service downtime can still be disruptive for some users. Next, if a service disruption happens unexpectedly, this may be the result of a team structure issue where developers and operators are working in silos. The structure of these teams restricts collaboration and obscures accountability. It doesn't help that these two groups also tend to have competing objectives. Developers are responsible for writing code for systems and applications, and operators are responsible for ensuring that those systems and applications operate reliably. Additionally, developers are expected to be agile and are often pushed to write and deploy code quickly. Their aim is to release new functions frequently, increase core business value with new features, and release fixes fast for an overall better user experience. In contrast, operators are expected to keep system stable, and so they often prefer to work more slowly to ensure reliability and consistency. Traditionally, developers would push their code to operators who often had little understanding of how the code would run in a production or live environment. When a problem does arise, it becomes very difficult for either group to identify the source of the problem and resolve it quickly. Worse, accountability between the teams is not always clear. So for organizations to thrive in the cloud, they need to adapt their IT operations in two ways. First, they'll need to adjust their expectations for service availability from 100% to a lower percentage. Second, they need to adopt best practices from the developer operations or DevOps, and site reliability engineering or SRE so teams can be more agile and work more collaboratively with clearer accountability. I'll examine service availability now and present DevOps and SRE in the next video. You might be thinking, "Why would any business leader "adjust their expectations for service availability? "Wouldn't they want customers to be able to access their online services 100% of the time?"

As I covered earlier, 100% availability is misleading. In order to roll out updates, operators have to take a system offline. Ensuring 100% service availability is also incredibly expensive for any business. This means that, at some point, the marginal cost of reliability exceeds the marginal value of reliability. To address this challenge, cloud providers use standard practices to define and measure service availability for customers. This practice includes a service level agreement, service level objectives, and service level indicators. A service level agreement or SLA is a contractual commitment between the cloud service provider and the customer. The SLA provides the baseline level for the quality, availability and reliability of that service. If the baseline service is not met by the provider, end users and end customers would be affected. In this case, the cloud provider would incur a cost usually paid out to the customer. A service level objective or SLO is a key element within the SLA. It's the goal for the cloud service performance level, and it's shared between the cloud provider and a customer if the service performance meets or exceeds the SLO, It means that end users, customers, and internal stakeholders are all happy. If the service performance is below the SLO and above the SLA or baseline performance expectation, it does not directly affect the end user or end customer, but it does give the cloud provider the signal to reduce service outages and increase service reliability instead of pushing out new updates. A service level indicator or SLI is a measure of the service provided. For example, SLIs often include reliability and errors. This brings me to another important term: error budget. An error budget is the amount of error that a service provider can accumulate over a certain period of time before end users start feeling unhappy. You can think of this as the pain tolerance for end users, but apply to a certain dimension of a service such as availability, latency, and so forth. The error budget is typically the space between the SLA and the SLO. This error budget gives developers clarity into how many failed fixes they can attempt without affecting the end user experience. By adjusting service performance expectations, with an SLA, SLO, SLIs, and error budgets, businesses can optimize their cloud environment and create better, more seamless customer experiences. In the next video, I'll explain what DevOps and site reliability are and how organizations can adopt best practices from each to adjust service availability expectations and improve their team's IT operations.




3.2 Google Cloud best practice and solutions

DevOps and SRE
When an organization needs to adapt their operations, team structure, IT leaders can use best practices from DevOps and site reliability engineering. Let's examine each of these, starting with DevOps. DevOps or developers operations is a philosophy that seeks to create a more collaborative and accountable culture within developer and operations teams. It's important to understand that all our DevOps philosophy highlights how IT teams can operate. DevOps doesn't give explicit guidance on how an organization should implement practices to be successful.

To understand the philosophy, Google has outlined five objectives of DevOps. First, it allows organizations to reduce silos, businesses can increase in foster collaboration by breaking down barriers across teams. Second, organizations need to accept failure as normal. Computers are inherently unreliable, so leaders can expect perfect execution. Third, organizations need to implement gradual change. Small incremental changes are easier to review and in the event that a gradual change does release a bug in production, it allows teams to reduce their time to recover, making it simple to roll back. Fourth, businesses should leverage tooling and automation. Identifying manual work that can be automated is key to working efficiently and focusing on the tasks that matter. And finally, organizations need to measure everything, measurement is a critical gauge for success. There's no way to tell whether what you're doing is successful if you have no way to measure it. While DevOps is a conceptual approach to collaborative and accountable data culture. SRE is about the practical implementation of that philosophy. Site reliability engineering or SRE is a discipline that applies aspects of software engineering to operations. The goals of SRE are to create ultra scalable and highly reliable software systems. SRE emerged organically as a job function within Google in the early 2000's, at the time, some software engineers were responsible for both writing code and running production systems. As a result, these engineers were renamed site reliability engineers. To better describe the kind of hybrid work they were doing. Not every organization that embraces these practices will use the job title of site reliability engineer or refer to site reliability engineering as a team function and that's okay. What's important are the best practices central to SRE. Let's look at the best practices now and how they align with DevOps objectives. Remember the five objectives of DevOps are to reduce organizational silos, accept failure as normal, implement gradual change, leverage, tooling and automation and measure everything.

Here's how these objectives aligned with SRE best practices. First, SRS emphasized the importance of establishing shared ownership of production between developers and operations to meet DevOps objective. One, reduce silos, together they defined service level objectives or SLOS, calculate error budgets and determine reliability and order work priorities. Culturally, the shared ownership promotes shared vision and knowledge and the need for improved collaboration and communications. Second, SRS believe that accepting failure as normal ie, the second, DevOps objective helps to build an iterative, collaborative culture.

One way this is done is by holding a blameless lessons learned discussion after an incident occurs. This practice helps SRS improve their understanding of system failures in order to identify preventative actions and ultimately reduce the likelihood or impact of a similar incident.

Third, when implementing gradual changes, which is the third DevOps objective. SRS aim to reduce the cost of failure by rolling out changes to a small percentage of users first. Culturally, this promotes more prototyping and launching iteratively. Next, in order to fulfill the fourth DevOps objective leverage tooling and automation, SRS focus on increasing efficiency through toil automation. In software engineering, Toil is a type of work that is tied to running a production service, Toil automation therefore reduces the amount of manual repetitive work. Finally, to meet the fifth DevOps objective measure everything. SRS recommend tracking everything related to toil, reliability and system health.

To foster these practices, organizations need a culture of goal setting, transparency and data driven decision making. They also need the tools to monitor their cloud environment and to identify whether they're meeting their service level objectives. SRE shifts the mindset from 100% availability to 99.99% or 99.99% availability. This means that updates are pushed out iterative lee and continually, but only require seconds or minutes of downtime. It's important to remember that DevOps and SRE have the same intended outcome. They're both designed to break down organizational barriers to help deliver better customer experiences faster. In the next video, I'll present a few google cloud solutions that help organizations measure everything and maintain visibility of their cloud environment.




Google Cloud resource monitoring tools
In a traditional on premises environment where IT teams can access their physical data centers whenever there's a service issue, these teams can physically examine their servers. For example, if an application becomes unresponsive, anyone can inspect the server to uncover the problem. When using a cloud service provider, the customer IT teams don't own the servers in use. The servers belong to the cloud provider. So how can customer IT teams know what's happening within a server or database or application? One option is to use the tools in Google Cloud's operations suite. Google Cloud's operations suite offer a range of services and cloud computing resources to help monitor, troubleshoot, and improve application performance on an organization's Google Cloud environment. The tools included in Google Cloud's operation suite fall into two major categories. The first is operations focus tools, which include cloud monitoring, cloud logging, error reporting, and service monitoring. These tools tend to be for users that want to keep their infrastructure up, running. and error free.

And the second is application performance management tools, which includes Cloud Debugger, Cloud Trace, and Cloud Profiler. In contrast, these features tend to be for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute services.

Let's cover a few of these solutions in more detail. The term monitoring refers to gathering predefined sets of metrics or logs. Cloud monitoring is the foundation for site reliability engineering because it provides visibility into the performance, uptime, and overall health of cloud powered applications.

Businesses can collect metrics, events, and metadata from Google Cloud services, visualize them on charts and dashboards, and manage alerts. Cloud monitoring also evaluates the performance of the entire infrastructure on a modular level. This means that properties such as server uptime, and response rate can help in evaluating customer experience.

Cloud Logging is another resource monitoring tool. A log file is a text file where applications including the operating system write events. Log files make it easier for developers, DevOps, and system admins to get insights and identify the root cause of issues within applications and the infrastructure. Google Cloud Logging is a fully managed service that performs at scale and can ingest application and system log data as well as custom log data from Google Kubernetes engine or GKE environments, virtual machines, and Google Cloud services. Cloud Logging allows IT teams to analyze selected logs and accelerate application troubleshooting. Both Cloud Logging and cloud monitoring provide IT operations teams with out of the box observability needed to monitor the IT infrastructure and applications. Cloud Logging automatically ingest Google Cloud audit and platform logs so that customers can get started right away. Cloud monitoring also provides a view of all Google Cloud metrics at zero costs and integrates with a variety of providers for non-Google cloud monitoring. All Google Cloud services from Google Kubernetes engine to BigQuery to Cloud Spanner stream metrics and logs into the Google Cloud Logging and cloud monitoring components. Cloud Debugger helps monitor application performance. In particular, IT teams can inspect the state of running application in real time without stopping or slowing it down. No downtime here. This means that end users are not affected while a developer searches the source code. IT teams can use it to understand the behavior of their code in production and analyze it's state to find those hard to find bugs.

Cloud Trace is another Google Cloud solution for monitoring application performance. When an application architecture is chunked into small pieces using either microservices or containers, or both, finding the source of a bug or problem can be challenging. Cloud Trace is a distributed tracing system that helps developers debug or fix and optimize their code. With Google Cloud's operations suite, organizations can trust that even without physical access to servers, they can still gain precise insight into their cloud IT environment. In fact, the features and benefits of these tools enable IT teams to do far more than they ever could with their on premises IT environment.

By running your cloud IT services, Google Cloud removes the inconvenience of maintaining servers, databases, and applications on premises all while still providing the tools and capabilities to source, analyze, and fix system errors quickly and effectively. And that is it for an overview of the third operational change category as companies start using cloud. Move on to the next video to review a summary of the key points in this course.




Summary
Let's take a few minutes now to recap the key points. Businesses can leverage cloud technology to serve their customers in radical new ways. But in order to do that, they need to evolve their IT operations and security to be optimized for the cloud environment. We explored three main operational shifts that organizations need to focus on in the cloud era. First, we saw how organizations no longer invest in building their own infrastructure. Instead, they rent storage space and purchase IT services from a public cloud provider. This allows organizations to manage spend based on cloud usage over time. Therefore, cost planning shifts from a capital expenditure model to an operational expenditure model. Second, we explored how security for on-premises IT relies on a perimeter model. With Google Cloud especially, security is built into every layer of the infrastructure stack, all the way down to the individual data chunks. The responsibility to secure data is shared between the cloud provider and the customer. Finally, we explored the mindset shift that business leaders and IT teams need to embrace to better monitor and control IT services and resources. By focusing on 99.99% service availability instead of 100% availability, businesses can roll out updates and innovative solutions without affecting the end user experience. By drawing from DevOps and Site Reliability Engineering, businesses can implement a more collaborative team structure. IT teams can also use best practices from these sources to better monitor their services and resources, debug system issues quickly, and release updates more efficiently. Adopting a new mindset and evolving operations in cost management, security, and IT team operations provides a foundation for organizations to push the boundaries with cloud technology and serve customers in a new and more effective ways. All right, that's it for this course. We can't wait to see what you'll come up with as you embark on your cloud adoption journey. For the full Google Cloud course catalog, visit cloud google.com/training. And if you signed up for the learning path, be sure to complete all four courses to receive credit.